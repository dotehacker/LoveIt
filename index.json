[{"categories":null,"content":"biography.","date":"2020-06-23","objectID":"/biography/","tags":null,"title":"Biography","uri":"/biography/"},{"categories":null,"content":"Affiliation Computer Engineering Student. Dept. Electronics and Computer Engineering,. Pulchowk Campus Tribhuvan University, Nepal ","date":"2020-06-23","objectID":"/biography/:1:0","tags":null,"title":"Biography","uri":"/biography/"},{"categories":null,"content":"Interests Data Manupulation (Machine Learning) Project Management Software Engineering (C++, Python) Photography ","date":"2020-06-23","objectID":"/biography/:2:0","tags":null,"title":"Biography","uri":"/biography/"},{"categories":null,"content":"Technical Skills C/C++ Python Numpy/Tensorflow ","date":"2020-06-23","objectID":"/biography/:3:0","tags":null,"title":"Biography","uri":"/biography/"},{"categories":null,"content":"Publications SUPPORT VECTORS ARE A BETTER WAY OF TEXT CLASSIFICATION FOR IMBALANCED DATA MACHINE LEARNING ANALYSIS OF TIRHUTA LIPI ","date":"2020-06-23","objectID":"/biography/:4:0","tags":null,"title":"Biography","uri":"/biography/"},{"categories":null,"content":"Lab Files Introduction to UNIX Commands and Shell Programming Lab Report on A Gentics Algorithm Lab Report on An Artificial Neural Network Lab Report on Constraint Programming Lab Report on Propositional Logic Familiarization with basic CT/DT functions see Publications. ","date":"2020-06-23","objectID":"/biography/:5:0","tags":null,"title":"Biography","uri":"/biography/"},{"categories":null,"content":"Educational/Work history 2022/06/01(End on 2023/06/01)~ Software Coordinator of PDSC 2019/08/01~ Pulchowk campus, Computer Engineering. 2017/04/01~ Kathmandu Bernhardt School, +2 level. 2007/03/01~ School Level. ","date":"2020-06-23","objectID":"/biography/:6:0","tags":null,"title":"Biography","uri":"/biography/"},{"categories":null,"content":"Courses GAN Specialization(Coursera). Deep Learning Specialization(Coursera). Deep Learning with Tensorflow (EDX) Machine Learning(Coursera). ","date":"2020-06-23","objectID":"/biography/:7:0","tags":null,"title":"Biography","uri":"/biography/"},{"categories":null,"content":"Honors and Awards Winner of GritFeat AI Hackathon 2023, Locus - Feb, 2022,(SWIFT’ is a wearable devices with hardware and AI models that detect falls in elderly people with 0.7986 accuracy, resulting in immediate emergency alerts to contacts.) Second RunnerUP of Dataverse, Locus - Jan, 2022,Dataverse Solution (NLP pased problem to classify abstract.) Winner of Best AI Project of Deltathon, DELTA 3.0 - Jan, 2022,Nepali Harvest (Designed a portal to help farmers that can predicting diseases, identifying optimal harvest times, and aiding with crop health assessment.) Winner of Image Challenge - AI Competition - Kathmandu University Computer Club - IT Meet 2022. Second Place - Docsumo DataRush, an AI and Data Science Competition 2022. Advance Level Winner- Capture The Flag 2022 AI and Robotics Member - RAN. Joint Secretary - NTBNS. ","date":"2020-06-23","objectID":"/biography/:8:0","tags":null,"title":"Biography","uri":"/biography/"},{"categories":null,"content":"Project Minor Project: Maithili_Lipi Used to classify Maithili_Lipi(Tirhuta) with high accurac, its begining of OCR for low resources language. IRB (Image Recognition Based) Robotics Arm (Image Processing, Signal Processing, Actuator Control): Research oriented, open source, Project under UN’s SDG3 - Good Health \u0026 Well-Being. Tech: Python, Arduino Programming, Arduino Toolkit, TensorFlow (May ’2020) Microsoft Rice Disease Classification Challenge (Computer Vision): AI model to classify Rice plant disease \u0026 fastly and efficently. Powered by ZINDI, LB score 0.077 and LB Postion 40. Tech: Python, Transformer(timm), Pytorch, Boosting Algorithm \u0026 Transfer learning. (August ’2022) Nepali Language (Devanagari Classifier, Nepali Sentiment Classifier, Nepali OCR Nepali Poem Generator): In devanagari letter classifier trained vgg16 model(acc. 0.94), To test accuracy of Nepali Language Transformer, i use nepali sentiment analysis score after fine tunning, and a simple OCR based on API of easyOCR Tech: Keras, Transformer, Pytorch, TF-IDF, NLTK (Past 2 Years) Unsupervised Model (VAE, GAN, C- GAN, AC-GAN, DC-GAN): Research oriented, Project for learning the behaviour of laten space and image manupulation. Tech: Python, Numpy, Tensorflow (Sep, 2021) NEPSE Simple(Web Development, Web Scraping, Telegram Bot): Created from scratch a nepal stock market data and present it in minimal enviroment constraint. Tech: Github Workflow, Automation in Scraping , HTML, CSS, WebSockets, JavaScript, RSS, XML ( Since 2020) ","date":"2020-06-23","objectID":"/biography/:9:0","tags":null,"title":"Biography","uri":"/biography/"},{"categories":null,"content":"Find my CV \u003e HERE. Excuse me, but this, this is just a piece of paper, If I’m going to be worthy of this institution, I will show you in action. –\u003eTom and Jerry(Kayla) ","date":"2020-06-23","objectID":"/biography/:10:0","tags":null,"title":"Biography","uri":"/biography/"},{"categories":null,"content":"photography.","date":"2020-06-23","objectID":"/photography/","tags":null,"title":"Photography","uri":"/photography/"},{"categories":null,"content":" PHOTO FROM MY LENS Flower and More ","date":"2020-06-23","objectID":"/photography/:0:0","tags":null,"title":"Photography","uri":"/photography/"},{"categories":null,"content":"Sun Flower Gradient Life and Colour. ","date":"2020-06-23","objectID":"/photography/:1:0","tags":null,"title":"Photography","uri":"/photography/"},{"categories":null,"content":"Madagascar Periwinkle Water and Flower. ","date":"2020-06-23","objectID":"/photography/:2:0","tags":null,"title":"Photography","uri":"/photography/"},{"categories":null,"content":"Pointed Nose Math never satify to paint. ","date":"2020-06-23","objectID":"/photography/:3:0","tags":null,"title":"Photography","uri":"/photography/"},{"categories":null,"content":"Red Flower Red colour is my Passion. ","date":"2020-06-23","objectID":"/photography/:4:0","tags":null,"title":"Photography","uri":"/photography/"},{"categories":null,"content":"Red Flower God is like you. ","date":"2020-06-23","objectID":"/photography/:5:0","tags":null,"title":"Photography","uri":"/photography/"},{"categories":null,"content":"All Time ❤️ Wide Angle View. ","date":"2020-06-23","objectID":"/photography/:6:0","tags":null,"title":"Photography","uri":"/photography/"},{"categories":null,"content":"Red Flower Sun and flower. ","date":"2020-06-23","objectID":"/photography/:7:0","tags":null,"title":"Photography","uri":"/photography/"},{"categories":null,"content":"Red Flower Frame and Sun. ","date":"2020-06-23","objectID":"/photography/:8:0","tags":null,"title":"Photography","uri":"/photography/"},{"categories":null,"content":"Red Flower Wild style. ","date":"2020-06-23","objectID":"/photography/:9:0","tags":null,"title":"Photography","uri":"/photography/"},{"categories":null,"content":"Red Flower Periwinkle. ","date":"2020-06-23","objectID":"/photography/:10:0","tags":null,"title":"Photography","uri":"/photography/"},{"categories":null,"content":"Red Flower Lord Hindu. ","date":"2020-06-23","objectID":"/photography/:11:0","tags":null,"title":"Photography","uri":"/photography/"},{"categories":null,"content":"Red Flower Focus to Friend. ","date":"2020-06-23","objectID":"/photography/:12:0","tags":null,"title":"Photography","uri":"/photography/"},{"categories":null,"content":"Red Flower Little Flower. ","date":"2020-06-23","objectID":"/photography/:13:0","tags":null,"title":"Photography","uri":"/photography/"},{"categories":null,"content":"Red Flower Show white. ","date":"2020-06-23","objectID":"/photography/:14:0","tags":null,"title":"Photography","uri":"/photography/"},{"categories":null,"content":"Red Flower Morning Blow. ","date":"2020-06-23","objectID":"/photography/:15:0","tags":null,"title":"Photography","uri":"/photography/"},{"categories":null,"content":"Red Flower For you. ","date":"2020-06-23","objectID":"/photography/:16:0","tags":null,"title":"Photography","uri":"/photography/"},{"categories":null,"content":"Red Flower Tiled Sun. ","date":"2020-06-23","objectID":"/photography/:17:0","tags":null,"title":"Photography","uri":"/photography/"},{"categories":null,"content":"Red Flower Moon and Colour. ","date":"2020-06-23","objectID":"/photography/:18:0","tags":null,"title":"Photography","uri":"/photography/"},{"categories":null,"content":"Red Flower Full moon in Red filter. ","date":"2020-06-23","objectID":"/photography/:19:0","tags":null,"title":"Photography","uri":"/photography/"},{"categories":null,"content":"Red Flower Random edit. ","date":"2020-06-23","objectID":"/photography/:20:0","tags":null,"title":"Photography","uri":"/photography/"},{"categories":null,"content":"Red Flower Painted Water. ","date":"2020-06-23","objectID":"/photography/:21:0","tags":null,"title":"Photography","uri":"/photography/"},{"categories":null,"content":"Red Flower Yellow Garden. ","date":"2020-06-23","objectID":"/photography/:22:0","tags":null,"title":"Photography","uri":"/photography/"},{"categories":null,"content":"Red Flower Medagascar. ","date":"2020-06-23","objectID":"/photography/:23:0","tags":null,"title":"Photography","uri":"/photography/"},{"categories":null,"content":"Red Flower Point Red. ","date":"2020-06-23","objectID":"/photography/:24:0","tags":null,"title":"Photography","uri":"/photography/"},{"categories":null,"content":"Red Flower RED and WHITE. ","date":"2020-06-23","objectID":"/photography/:25:0","tags":null,"title":"Photography","uri":"/photography/"},{"categories":null,"content":"Red Flower White flower. ","date":"2020-06-23","objectID":"/photography/:26:0","tags":null,"title":"Photography","uri":"/photography/"},{"categories":null,"content":"Red Flower This photo is captured at janakpur. ","date":"2020-06-23","objectID":"/photography/:27:0","tags":null,"title":"Photography","uri":"/photography/"},{"categories":null,"content":"Red Flower On your feet. ","date":"2020-06-23","objectID":"/photography/:28:0","tags":null,"title":"Photography","uri":"/photography/"},{"categories":null,"content":"Red Flower Hindu trilogy. ","date":"2020-06-23","objectID":"/photography/:29:0","tags":null,"title":"Photography","uri":"/photography/"},{"categories":null,"content":"Red Flower Path to Heaven. ","date":"2020-06-23","objectID":"/photography/:30:0","tags":null,"title":"Photography","uri":"/photography/"},{"categories":null,"content":"Red Flower Wild Morning. ","date":"2020-06-23","objectID":"/photography/:31:0","tags":null,"title":"Photography","uri":"/photography/"},{"categories":null,"content":"Red Flower Snow flower. ","date":"2020-06-23","objectID":"/photography/:32:0","tags":null,"title":"Photography","uri":"/photography/"},{"categories":null,"content":"Red Flower Colour more. ","date":"2020-06-23","objectID":"/photography/:33:0","tags":null,"title":"Photography","uri":"/photography/"},{"categories":null,"content":"Red Flower Scattered Flower. ","date":"2020-06-23","objectID":"/photography/:34:0","tags":null,"title":"Photography","uri":"/photography/"},{"categories":null,"content":"Mustard Flower Yellow Family ","date":"2020-06-23","objectID":"/photography/:35:0","tags":null,"title":"Photography","uri":"/photography/"},{"categories":null,"content":"MariGold Red and Green ","date":"2020-06-23","objectID":"/photography/:36:0","tags":null,"title":"Photography","uri":"/photography/"},{"categories":null,"content":"White Flower Dusty Background ","date":"2020-06-23","objectID":"/photography/:37:0","tags":null,"title":"Photography","uri":"/photography/"},{"categories":null,"content":"Morning Girl WET!! ","date":"2020-06-23","objectID":"/photography/:38:0","tags":null,"title":"Photography","uri":"/photography/"},{"categories":null,"content":"Pink Flower Same as ME ","date":"2020-06-23","objectID":"/photography/:39:0","tags":null,"title":"Photography","uri":"/photography/"},{"categories":null,"content":"Leaves and Flower Green and Red ","date":"2020-06-23","objectID":"/photography/:40:0","tags":null,"title":"Photography","uri":"/photography/"},{"categories":null,"content":"Janaki Mandir Protected by Cloud of LOVE. ","date":"2020-06-23","objectID":"/photography/:41:0","tags":null,"title":"Photography","uri":"/photography/"},{"categories":null,"content":"Our Dynasty Krishna Mandir. ","date":"2020-06-23","objectID":"/photography/:42:0","tags":null,"title":"Photography","uri":"/photography/"},{"categories":null,"content":"Birth Place oF Maa Janaki Lockdown View. ","date":"2020-06-23","objectID":"/photography/:43:0","tags":null,"title":"Photography","uri":"/photography/"},{"categories":null,"content":"pulchowk Campus WhiteHouse Architecture Flower in Pcampus ","date":"2020-06-23","objectID":"/photography/:44:0","tags":null,"title":"Photography","uri":"/photography/"},{"categories":null,"content":"vim","date":"2021-08-27","objectID":"/vim/","tags":null,"title":"VIM","uri":"/vim/"},{"categories":null,"content":" VIM IT IS MODAL EDITOR(DIFFERENT OPERATING MODE) Normal mode(navigating around file) (i)Insert mode(esc) (r)replace mode(esc) (v)visual mode(esc) line mode(keyboard control) black mode(mouse control) ","date":"2021-08-27","objectID":"/vim/:0:0","tags":null,"title":"VIM","uri":"/vim/"},{"categories":null,"content":"code Normal mode :q —\u003e to quit a windows :q! —\u003e to quit entire vim :w —\u003e to save file :w *.txt —\u003e to save new file /words —\u003e to search the words in that particular file n —\u003e to select the next words of matched pattern. . —\u003e to repeat the same editing. ci —\u003e to edit the words inside the paranthesis. h —\u003e to move cursor left l —\u003e to move cursor right j —\u003e to move cursor down k —\u003e to move cursor up u —\u003e undo the change y —\u003e to copy the character yw —\u003e to copy the words p —\u003e to past the words v —\u003e to enable visual mode of selection of text o —\u003e to open a line for editing b —\u003e backspace the cursor by a words G —\u003e to move at the end of the words g —\u003e to move at the first of the words dw —\u003e to delete the words ce —\u003e to delete the word and back to insert mode x —\u003e to delete the character r —\u003e to replace the words Insert mode To move from Normal mode to Insert mode you can do following things. press ’s' press ‘i’ presh ‘gh’ To escape back to normal mode you may use ’esc’ key. ","date":"2021-08-27","objectID":"/vim/:1:0","tags":null,"title":"VIM","uri":"/vim/"},{"categories":["myth"],"content":"Discover what the Hugo - LoveIt theme is all about and the core-concepts behind it.","date":"2023-10-19","objectID":"/mahadev/","tags":["myth","god"],"title":"Mahadev","uri":"/mahadev/"},{"categories":["myth"],"content":"In a village, Shiva ruled as the king and held sway over his kingdom. However, over time, two influential individuals arrived from another land. They were known as Vishnu and Brahma, each having their own kingdom in their homeland. These newcomers quickly earned a reputation in the village as powerful kings. As time passed, the three rulers contemplated the idea of determining who among them would be considered the “Mahadevta” or “Mahadev,” a significant title often associated with Lord Shiva. Vishnu had already been celebrated for his divine incarnations in other lands, leaving Brahma and Shiva as the remaining contenders for the title. After extensive discussions, it seemed that Brahma was on the verge of becoming the Mahadev. His excitement got the better of him, and he lost control over his desires. This situation was only brought under control when his daughter, Saraswati, intervened. However, this event led to the villagers’ displeasure, and the position of Mahadev was transferred to Shiva. From that day onwards, Shiva was revered as the Mahadev, while Brahma was not as prominently worshipped. ","date":"2023-10-19","objectID":"/mahadev/:0:0","tags":["myth","god"],"title":"Mahadev","uri":"/mahadev/"},{"categories":["Festival"],"content":"Jur Shital: A Celebration of Water and Life in Mithila","date":"2023-04-14","objectID":"/jur_shital/","tags":["mithila","Jur SHital"],"title":"Jur Shital: A Celebration of Water and Life in Mithila","uri":"/jur_shital/"},{"categories":["Festival"],"content":" Jur Shital is a vibrant and colorful festival celebrated by the people of Mithila region, which comprises parts of India and Nepal. This festival is celebrated on the 2nd of Baisakh, the second month of the Hindu calendar. Jur Shital is a festival that celebrates water and life, and it is a time when people come together to pray for good health, prosperity, and longevity. One of the most interesting aspects of Jur Shital is the tradition of watering the head of the younger members of the family. In this tradition, the seniors in the family pour water on the heads of the younger members, wishing them long life and the ability to remain calm and peaceful like water throughout the year. This act symbolizes the importance of water in sustaining life and the need for peace and tranquility in our lives. Another important aspect of Jur Shital is the practice of watering all the plants and animals around them. This ritual signifies the importance of taking care of nature and the environment, which is necessary for the survival of all living beings. This ritual also demonstrates the belief that all life forms are interconnected and that we must take care of our environment to ensure a healthy and sustainable future for ourselves and our planet. Jur Shital is also famous for the diverse variety of vegetarian dishes that people prepare on this day. People prepare as many as 56 different types of vegetarian dishes, which they serve on a leaf of Makhan. These dishes are made with locally sourced and seasonal ingredients and reflect the rich culinary traditions of the Mithila region. The day of Jur Shital starts with people taking a bath early in the morning, followed by the tradition of pouring water on the heads of family members. After this, families gather together to enjoy the sumptuous vegetarian feast that is prepared with great care and devotion. In conclusion, Jur Shital is a festival that celebrates the importance of water, life, and nature. It is a time when people come together to pray for good health, prosperity, and longevity, and to enjoy the delicious vegetarian dishes that are an integral part of this festival. The rich cultural traditions of the Mithila region are reflected in this festival, making it a truly unique and beautiful celebration. ","date":"2023-04-14","objectID":"/jur_shital/:0:0","tags":["mithila","Jur SHital"],"title":"Jur Shital: A Celebration of Water and Life in Mithila","uri":"/jur_shital/"},{"categories":["ChatGPT Report"],"content":"Functional dependency explained. ","date":"2023-04-09","objectID":"/functional-dependency/:0:0","tags":["ChatGPT","Functional dependency"],"title":"Functional dependency","uri":"/functional-dependency/"},{"categories":["ChatGPT Report"],"content":"You Functional dependency ","date":"2023-04-09","objectID":"/functional-dependency/:0:1","tags":["ChatGPT","Functional dependency"],"title":"Functional dependency","uri":"/functional-dependency/"},{"categories":["ChatGPT Report"],"content":"ChatGPT In the context of relational databases, a functional dependency is a relationship between two or more attributes (or columns) in a table where the value of one attribute determines the value of another. For example, if we have a table called “Students” with the columns “Student_ID,” “First_Name,” “Last_Name,” and “Email,” we can say that the attribute “Email” is functionally dependent on the attributes “First_Name” and “Last_Name.” This is because the value of the “Email” attribute is determined by the values of the “First_Name” and “Last_Name” attributes. In general, a functional dependency is represented as A -\u003e B, where A is the determinant (or independent) attribute and B is the dependent attribute. This means that for any given value of A, there is only one possible value of B. Functional dependencies are important in database design because they help ensure data integrity and minimize redundancy. By identifying and removing redundant data, we can make our databases more efficient and easier to manage. ","date":"2023-04-09","objectID":"/functional-dependency/:0:2","tags":["ChatGPT","Functional dependency"],"title":"Functional dependency","uri":"/functional-dependency/"},{"categories":["ChatGPT Report"],"content":"You describe its type too with example ","date":"2023-04-09","objectID":"/functional-dependency/:0:3","tags":["ChatGPT","Functional dependency"],"title":"Functional dependency","uri":"/functional-dependency/"},{"categories":["ChatGPT Report"],"content":"ChatGPT There are several types of functional dependencies in a database. The main types are: Trivial Functional Dependency: A functional dependency A -\u003e B is trivial if B is a subset of A. For example, in a table with columns “Student_ID,” “First_Name,” and “Last_Name,” the functional dependency “Student_ID, First_Name -\u003e First_Name” is trivial because First_Name is already a part of the determinant. Non-Trivial Functional Dependency: A functional dependency A -\u003e B is non-trivial if B is not a subset of A. For example, in a table with columns “Student_ID,” “First_Name,” “Last_Name,” and “Email,” the functional dependency “First_Name, Last_Name -\u003e Email” is non-trivial because the Email attribute is not a part of the determinant. Transitive Functional Dependency: A functional dependency A -\u003e B and B -\u003e C implies a transitive functional dependency A -\u003e C. For example, in a table with columns “Student_ID,” “First_Name,” “Last_Name,” and “City,” the functional dependency “Student_ID -\u003e City” and “City -\u003e State” imply a transitive functional dependency “Student_ID -\u003e State”. Partial Dependency: A functional dependency A -\u003e B is a partial dependency if B is functionally dependent on a proper subset of A. For example, in a table with columns “Student_ID,” “First_Name,” “Last_Name,” and “Email,” the functional dependency “Student_ID, First_Name, Last_Name -\u003e Email” is a partial dependency because the attribute Email is also functionally dependent on just the subset “Student_ID”. Full Dependency: A functional dependency A -\u003e B is a full dependency if B is functionally dependent on the entire set of A. For example, in a table with columns “Student_ID,” “First_Name,” “Last_Name,” and “Email,” the functional dependency “Student_ID, First_Name, Last_Name -\u003e Email” is a full dependency because the attribute Email is functionally dependent on the entire set of “Student_ID”, “First_Name”, and “Last_Name”. ","date":"2023-04-09","objectID":"/functional-dependency/:0:4","tags":["ChatGPT","Functional dependency"],"title":"Functional dependency","uri":"/functional-dependency/"},{"categories":["ChatGPT Report"],"content":"You Normalizations and its forms, with example and tabels ","date":"2023-04-09","objectID":"/functional-dependency/:0:5","tags":["ChatGPT","Functional dependency"],"title":"Functional dependency","uri":"/functional-dependency/"},{"categories":["ChatGPT Report"],"content":"ChatGPT Normalization is the process of organizing data in a database to reduce data redundancy and dependency, making the database more efficient, easier to maintain, and less prone to errors. There are different normal forms in which data can be organized. The most commonly used normal forms are: First Normal Form (1NF): A table is in 1NF if it has no repeating groups or arrays, meaning that each column of the table contains only atomic values. For example, consider the following table called “Student_Info”: Student_ID First_Name Last_Name Course_List 1 John Smith Math, Science, English 2 Jane Doe Math, Science, History, French This table is not in 1NF because the “Course_List” column contains multiple values, which can be split into separate rows. We can normalize this table to 1NF by splitting the “Course_List” column into separate rows and creating a new table called “Course_Info”: Student_ID First_Name Last_Name 1 John Smith 2 Jane Doe Student_ID Course 1 Math 1 Science 1 English 2 Math 2 Science 2 History 2 French Second Normal Form (2NF): A table is in 2NF if it is in 1NF and every non-key attribute is fully functionally dependent on the primary key. For example, consider the following table called “Order_Details”: Order_ID Product_ID Product_Name Product_Price Quantity 1001 P001 iPhone X 999 2 1001 P002 AirPods 149 3 1002 P001 iPhone X 999 1 1002 P003 MacBook Pro 1499 1 This table is not in 2NF because the “Product_Name” and “Product_Price” columns are dependent only on the “Product_ID” column, which is not a primary key. We can normalize this table to 2NF by creating a new table called “Product_Info”: Product_ID Product_Name Product_Price P001 iPhone X 999 P002 AirPods 149 P003 MacBook Pro 1499 Order_ID Product_ID Quantity 1001 P001 2 1001 P002 3 1002 P001 1 1002 P003 1 Third Normal Form (3NF): A table is in 3NF if it is in 2NF and has no transitive dependencies. For example, consider the following table called “Employee_Details”: Employee_ID Employee_Name Department_ID Department_Name Manager_ID E001 John Smith D001 Sales ","date":"2023-04-09","objectID":"/functional-dependency/:0:6","tags":["ChatGPT","Functional dependency"],"title":"Functional dependency","uri":"/functional-dependency/"},{"categories":["ChatGPT Report"],"content":"You complete from Third Normal Form ","date":"2023-04-09","objectID":"/functional-dependency/:0:7","tags":["ChatGPT","Functional dependency"],"title":"Functional dependency","uri":"/functional-dependency/"},{"categories":["ChatGPT Report"],"content":"ChatGPT Continuing from the previous answer: Employee_ID Employee_Name Department_ID Manager_ID E001 John Smith D001 M001 E002 Jane Doe D001 M001 E003 Bob Johnson D002 M002 Department_ID Department_Name Manager_ID D001 Sales M001 D002 Marketing M002 Manager_ID Manager_Name M001 Jack Johnson M002 Sarah Thompson This table is not in 3NF because the “Department_Name” and “Manager_Name” columns are not directly dependent on the primary key “Employee_ID”. We can normalize this table to 3NF by creating new tables called “Department_Info” and “Manager_Info”: Employee_ID Employee_Name Department_ID E001 John Smith D001 E002 Jane Doe D001 E003 Bob Johnson D002 Department_ID Manager_ID D001 M001 D002 M002 Manager_ID Manager_Name M001 Jack Johnson M002 Sarah Thompson Department_ID Department_Name D001 Sales D002 Marketing This new design eliminates the transitive dependency and breaks down the original table into smaller, more manageable tables. ","date":"2023-04-09","objectID":"/functional-dependency/:0:8","tags":["ChatGPT","Functional dependency"],"title":"Functional dependency","uri":"/functional-dependency/"},{"categories":["ChatGPT Report"],"content":"You and give for BCNF ","date":"2023-04-09","objectID":"/functional-dependency/:0:9","tags":["ChatGPT","Functional dependency"],"title":"Functional dependency","uri":"/functional-dependency/"},{"categories":["ChatGPT Report"],"content":"ChatGPT Boyce-Codd Normal Form (BCNF) is a stricter form of normalization that ensures that every determinant in a table is a candidate key. A determinant is any attribute or combination of attributes that determines the values of other attributes in a table. A candidate key is a minimal set of attributes that can uniquely identify each row in a table. A table is in BCNF if and only if every determinant is a candidate key. For example, consider the following table called “Employee_Project”: Employee_ID Project_ID Project_Name Department_ID E001 P001 Project A D001 E001 P002 Project B D001 E002 P001 Project A D001 E003 P003 Project C D002 In this table, the determinant is the combination of “Employee_ID” and “Project_ID”, and it determines the values of “Project_Name” and “Department_ID”. However, “Department_ID” is not dependent on the candidate key “Employee_ID” alone, but on the combination of “Employee_ID” and “Project_ID”. Therefore, this table is not in BCNF. To normalize this table to BCNF, we can create two new tables called “Employee_Project_Info” and “Department_Info”: Employee_ID Project_ID E001 P001 E001 P002 E002 P001 E003 P003 Project_ID Project_Name P001 Project A P002 Project B P003 Project C Department_ID Project_ID D001 P001 D001 P002 D002 P003 Now, each table is in BCNF as each determinant is a candidate key. This design also eliminates data redundancy and dependency, making the database more efficient and easier to maintain. ","date":"2023-04-09","objectID":"/functional-dependency/:0:10","tags":["ChatGPT","Functional dependency"],"title":"Functional dependency","uri":"/functional-dependency/"},{"categories":["ChatGPT Report"],"content":"You difference between 3NF and BCNF with suitable example ","date":"2023-04-09","objectID":"/functional-dependency/:0:11","tags":["ChatGPT","Functional dependency"],"title":"Functional dependency","uri":"/functional-dependency/"},{"categories":["ChatGPT Report"],"content":"ChatGPT Both Third Normal Form (3NF) and Boyce-Codd Normal Form (BCNF) are forms of database normalization that aim to eliminate data redundancy and dependency. However, BCNF is a stricter form of normalization than 3NF. In 3NF, a table must satisfy the following conditions: It is in second normal form (2NF) It does not have transitive dependencies A transitive dependency is a functional dependency between two non-key attributes, where one non-key attribute depends on another non-key attribute, which in turn depends on the primary key. For example, consider the following table called “Customer_Orders”: Customer_ID Order_ID Order_Date Product_Name Product_Price C001 O001 2021-01-01 Product A $10 C001 O001 2021-01-01 Product B $20 C002 O002 2021-01-02 Product C $30 In this table, “Product_Name” and “Product_Price” are dependent on “Order_ID”, but “Order_ID” is not a candidate key. Instead, “Customer_ID” and “Order_ID” together form a composite primary key. Therefore, this table is not in 3NF as it has a transitive dependency between “Order_ID” and “Product_Name”/“Product_Price”. To normalize this table to 3NF, we can create two new tables called “Order_Info” and “Product_Info”: Customer_ID Order_ID Order_Date C001 O001 2021-01-01 C002 O002 2021-01-02 Order_ID Product_Name O001 Product A O001 Product B O002 Product C Product_Name Product_Price Product A $10 Product B $20 Product C $30 On the other hand, in BCNF, a table must satisfy the following condition: Every determinant is a candidate key A determinant is any attribute or combination of attributes that determines the values of other attributes in a table. A candidate key is a minimal set of attributes that can uniquely identify each row in a table. For example, consider the following table called “Employee_Project”: Employee_ID Project_ID Project_Name Department_ID E001 P001 Project A D001 E001 P002 Project B D001 E002 P001 Project A D001 E003 P003 Project C D002 In this table, the determinant is the combination of “Employee_ID” and “Project_ID”, and it determines the values of “Project_Name” and “Department_ID”. However, “Department_ID” is not dependent on the candidate key “Employee_ID” alone, but on the combination of “Employee_ID” and “Project_ID”. Therefore, this table is not in BCNF. To normalize this table to BCNF, we can create two new tables called “Employee_Project_Info” and “Department_Info”, as shown in the previous example. In summary, the main difference between 3NF and BCNF is that BCNF ","date":"2023-04-09","objectID":"/functional-dependency/:0:12","tags":["ChatGPT","Functional dependency"],"title":"Functional dependency","uri":"/functional-dependency/"},{"categories":["ChatGPT Report"],"content":"You In summary, the main difference between 3NF and BCNF is that BCNF ","date":"2023-04-09","objectID":"/functional-dependency/:0:13","tags":["ChatGPT","Functional dependency"],"title":"Functional dependency","uri":"/functional-dependency/"},{"categories":["ChatGPT Report"],"content":"ChatGPT requires every determinant in a table to be a candidate key, whereas 3NF only requires that a table does not have transitive dependencies. BCNF is a stricter form of normalization than 3NF and leads to a more optimized database design. ","date":"2023-04-09","objectID":"/functional-dependency/:0:14","tags":["ChatGPT","Functional dependency"],"title":"Functional dependency","uri":"/functional-dependency/"},{"categories":["ChatGPT Report"],"content":" OPerating system and function 1 / 1 An operating system (OS) is a software system that manages computer hardware and software resources and provides common services for computer programs. The main functions of an operating system include: Process management: The OS manages the execution of computer programs, called processes, by allocating system resources such as memory, CPU time, and input/output devices. Memory management: The OS manages the allocation and deallocation of memory resources to processes, ensuring that each process has the necessary memory to execute. File management: The OS manages files and directories on the storage devices attached to the computer, providing services such as file creation, deletion, and access control. Device management: The OS manages the input/output devices attached to the computer, including keyboards, mice, displays, printers, and storage devices, ensuring that processes can communicate with these devices. Security management: The OS provides mechanisms for controlling access to system resources, protecting the system from unauthorized access and malicious software. Network management: The OS provides services for communicating with other computers and networks, including protocols for sending and receiving data over the internet. Overall, the operating system is responsible for managing the resources of the computer system and providing a platform for the execution of software applications. 2 / 2 describe all its function in details 1 / 1 Sure, I’ll describe each function of an operating system in more detail: Process Management: Process management is one of the core functions of an operating system. It involves managing the execution of computer programs or processes. The OS allocates system resources such as memory, CPU time, and input/output devices to each process, ensuring that all processes can execute efficiently. The OS also schedules processes based on priority, time-sharing, or other criteria, to ensure that all processes get a fair share of system resources. It also provides mechanisms for interprocess communication and synchronization, allowing processes to communicate with each other and share resources when needed. Memory Management: Memory management is another important function of an operating system. It involves managing the allocation and deallocation of memory resources to processes. The OS ensures that each process has the necessary memory to execute, and it manages the allocation and deallocation of memory to optimize system performance. The OS also provides virtual memory, which allows processes to use more memory than is physically available by using disk space as an extension of main memory. This helps to prevent system crashes due to insufficient memory. File Management: File management is the function of an operating system that manages files and directories on the storage devices attached to the computer. The OS provides services such as file creation, deletion, renaming, copying, and access control. The OS also provides file system drivers, which allow the computer to read and write data to different types of storage devices, such as hard drives, flash drives, and CD/DVD drives. The file system driver translates file requests from the OS into commands that can be understood by the storage device. Device Management: Device management is the function of an operating system that manages the input/output devices attached to the computer, including keyboards, mice, displays, printers, and storage devices. The OS provides device drivers, which are software components that enable the computer to communicate with the devices. The device drivers translate requests from the OS into commands that can be understood by the device, and they also manage the flow of data between the device and the computer. The OS also provides mechanisms for configuring and managing devices, such as device discovery, device installation, and device removal. Security Management: Securit","date":"2023-04-09","objectID":"/operating_system/:0:0","tags":["ChatGPT","Operating_System"],"title":"Operating_System","uri":"/operating_system/"},{"categories":["ChatGPT Report"],"content":"Query optimization techniques. ","date":"2023-04-09","objectID":"/queryoptimizationtechniques/:0:0","tags":["ChatGPT","Query optimization"],"title":"Query optimization","uri":"/queryoptimizationtechniques/"},{"categories":["ChatGPT Report"],"content":"You query peocessing and optimization ","date":"2023-04-09","objectID":"/queryoptimizationtechniques/:0:1","tags":["ChatGPT","Query optimization"],"title":"Query optimization","uri":"/queryoptimizationtechniques/"},{"categories":["ChatGPT Report"],"content":"ChatGPT Query processing and optimization are two important aspects of database management systems that deal with the execution of user queries on a database. Query processing involves transforming a user query written in a high-level language (such as SQL) into a series of lower-level operations that can be executed by the database system. The process of query processing typically involves parsing the query, checking it for syntactic and semantic errors, optimizing the query execution plan, and executing the query to retrieve the desired data. Query optimization involves finding the most efficient way to execute a user query. The goal of query optimization is to minimize the cost of executing a query, which includes factors such as disk I/O, CPU usage, and memory usage. The optimizer will consider various alternative execution plans and choose the one that has the lowest estimated cost. Some of the techniques used for query optimization include index selection, join ordering, and aggregation optimization. In addition, many modern database management systems also support advanced optimization techniques such as parallel processing, materialized views, and query rewriting. Overall, query processing and optimization are critical components of a high-performance database system, and the effectiveness of these techniques can have a significant impact on the performance and scalability of a database application. ","date":"2023-04-09","objectID":"/queryoptimizationtechniques/:0:2","tags":["ChatGPT","Query optimization"],"title":"Query optimization","uri":"/queryoptimizationtechniques/"},{"categories":["ChatGPT Report"],"content":"You basic steps of query processing ","date":"2023-04-09","objectID":"/queryoptimizationtechniques/:0:3","tags":["ChatGPT","Query optimization"],"title":"Query optimization","uri":"/queryoptimizationtechniques/"},{"categories":["ChatGPT Report"],"content":"ChatGPT The basic steps of query processing in a database system can be summarized as follows: Parsing: The first step in query processing is parsing the user query to ensure that it is syntactically correct and meaningful. During parsing, the query is checked for syntax errors and parsed into a parse tree or an equivalent data structure. Semantic Analysis: Once the query has been parsed, semantic analysis is performed to check the query for semantic errors, such as checking for the existence of the referenced tables and columns. Optimization: After the query has been parsed and validated, the database system tries to optimize the query by generating an efficient execution plan. The optimization process involves selecting an appropriate join order, selecting the most efficient indexes, and determining which filters to apply to reduce the data size. Execution: After the query has been optimized, the actual execution of the query begins. The execution plan generated by the optimizer is followed to retrieve the desired data from the database. Result Retrieval: Once the query execution is complete, the results are retrieved and returned to the user. This step involves retrieving the data from the database and formatting it in the desired format, such as a table or a chart. Query Caching: In some cases, the database system may cache the query result to avoid the overhead of executing the query again for the same input parameters. This can improve the performance of frequently executed queries and reduce the load on the database system. Overall, query processing is a complex process that involves many steps, and the performance of the database system depends heavily on the efficiency of each step. By optimizing the query processing steps, database systems can improve the performance and scalability of their applications. ","date":"2023-04-09","objectID":"/queryoptimizationtechniques/:0:4","tags":["ChatGPT","Query optimization"],"title":"Query optimization","uri":"/queryoptimizationtechniques/"},{"categories":["ChatGPT Report"],"content":"You explain by examples ","date":"2023-04-09","objectID":"/queryoptimizationtechniques/:0:5","tags":["ChatGPT","Query optimization"],"title":"Query optimization","uri":"/queryoptimizationtechniques/"},{"categories":["ChatGPT Report"],"content":"ChatGPT Sure, I can explain each step of the query processing using a simple example. Let’s assume we have a database with a table called “Students” that contains the following columns: ID: Student ID Name: Student name Age: Student age Major: Student major And we want to retrieve the names and majors of all students who are 20 years old. Parsing: The first step in query processing is parsing the user query to ensure that it is syntactically correct and meaningful. In our example, the user query is: SELECT Name, Major FROM Students WHERE Age = 20 The parser checks that the query syntax is correct and generates a parse tree that represents the query structure. Semantic Analysis: Once the query has been parsed, semantic analysis is performed to check the query for semantic errors. In our example, the semantic analyzer checks that the referenced tables and columns exist in the database and are accessible by the user. Optimization: After the query has been parsed and validated, the database system tries to optimize the query by generating an efficient execution plan. In our example, the optimizer determines that an index on the “Age” column would be helpful to filter the data. The optimizer also determines that a table scan should be used to retrieve the data because the table is small, and using an index would be less efficient. Execution: After the query has been optimized, the actual execution of the query begins. The execution plan generated by the optimizer is followed to retrieve the desired data from the database. In our example, the database system scans the entire “Students” table to find all records with an Age value of 20. Result Retrieval: Once the query execution is complete, the results are retrieved and returned to the user. In our example, the database system returns the names and majors of all students who are 20 years old, which may look like: Name Major John Computer Science Mary Biology Alice Math Query Caching: In some cases, the database system may cache the query result to avoid the overhead of executing the query again for the same input parameters. In our example, if the user executes the same query again with the same input parameter (Age = 20), the database system may retrieve the cached result instead of executing the query again. Overall, this is a basic example of the query processing steps involved in retrieving data from a database table. ","date":"2023-04-09","objectID":"/queryoptimizationtechniques/:0:6","tags":["ChatGPT","Query optimization"],"title":"Query optimization","uri":"/queryoptimizationtechniques/"},{"categories":["ChatGPT Report"],"content":"You describe Parser and translation ","date":"2023-04-09","objectID":"/queryoptimizationtechniques/:0:7","tags":["ChatGPT","Query optimization"],"title":"Query optimization","uri":"/queryoptimizationtechniques/"},{"categories":["ChatGPT Report"],"content":"ChatGPT Parser and translation are two important steps in the process of converting a user’s query into a form that can be executed by a database system. Parser: A parser is a software component that reads the user’s query and checks whether it conforms to the syntax rules of the query language. The parser’s primary goal is to ensure that the query is syntactically correct and can be interpreted by the database system. If the query contains syntax errors, the parser will generate an error message that describes the problem. During the parsing process, the parser breaks the query into smaller, meaningful components, such as keywords, operators, and identifiers, and creates a data structure that represents the query’s structure. This data structure is called a parse tree or an abstract syntax tree (AST) and is used as input for the translation step. Translation: The translation step converts the parse tree generated by the parser into an internal representation that can be executed by the database system. This internal representation may be a set of instructions for a virtual machine or an executable code for a specific database engine. The translation step also performs semantic analysis, which checks the query’s meaning and ensures that it is semantically correct. Semantic analysis involves verifying that the tables and columns referenced in the query exist in the database and that the query’s syntax conforms to the database schema. Finally, the translation step optimizes the query by choosing the most efficient query execution plan. The execution plan describes how the database system should retrieve and manipulate the data to produce the query result. Overall, the parser and translation steps are essential in converting a user’s query into a format that the database system can execute. By ensuring that the query is syntactically and semantically correct and optimizing the query execution plan, the parser and translation steps help to improve the performance and efficiency of database queries. ","date":"2023-04-09","objectID":"/queryoptimizationtechniques/:0:8","tags":["ChatGPT","Query optimization"],"title":"Query optimization","uri":"/queryoptimizationtechniques/"},{"categories":["ChatGPT Report"],"content":"You optimizer ","date":"2023-04-09","objectID":"/queryoptimizationtechniques/:0:9","tags":["ChatGPT","Query optimization"],"title":"Query optimization","uri":"/queryoptimizationtechniques/"},{"categories":["ChatGPT Report"],"content":"ChatGPT The optimizer is a component of a database management system (DBMS) that is responsible for selecting the most efficient way to execute a database query. Its goal is to minimize the amount of time and resources needed to retrieve the requested data from the database. When a user submits a query to the DBMS, the optimizer analyzes the query and generates one or more execution plans, which are sequences of operations that the DBMS can use to retrieve the requested data. The optimizer evaluates each execution plan and selects the one that it believes will require the least amount of time and resources. The optimizer considers many factors when generating execution plans, including: The size and complexity of the tables involved in the query. The type and number of joins required to retrieve the data. The availability and distribution of indexes that can be used to speed up data retrieval. The storage and access methods used by the database. The cost of data transfers and disk access. The optimizer may use a variety of algorithms to generate execution plans, including heuristics, cost-based methods, and rule-based methods. Heuristics are simple, rule-of-thumb algorithms that use fixed rules to generate execution plans. Cost-based methods use statistical information about the database to estimate the cost of different execution plans and select the one with the lowest estimated cost. Rule-based methods use a set of predefined rules to generate execution plans based on the structure of the query. The optimizer is an important component of a DBMS because it can significantly affect the performance of the database. An efficient optimizer can speed up query processing and reduce the amount of resources required to retrieve data, while an inefficient optimizer can lead to slow query processing and high resource usage. ","date":"2023-04-09","objectID":"/queryoptimizationtechniques/:0:10","tags":["ChatGPT","Query optimization"],"title":"Query optimization","uri":"/queryoptimizationtechniques/"},{"categories":["ChatGPT Report"],"content":"You query cost optimizaion ","date":"2023-04-09","objectID":"/queryoptimizationtechniques/:0:11","tags":["ChatGPT","Query optimization"],"title":"Query optimization","uri":"/queryoptimizationtechniques/"},{"categories":["ChatGPT Report"],"content":"ChatGPT Query cost optimization is a process of optimizing the execution of a database query in order to minimize the cost of processing the query, such as the amount of CPU time, disk I/O, and network bandwidth required to complete the query. The goal of query cost optimization is to execute a given query in the most efficient way possible, while producing the correct results. The query cost optimization process involves several steps, including: Query parsing and translation: The query is first parsed and translated into an internal representation, such as an abstract syntax tree (AST) or a relational algebra expression. This representation is used as input to the query optimizer. Query optimization: The query optimizer analyzes the query and generates one or more execution plans that could be used to execute the query. The optimizer evaluates each execution plan and chooses the one with the lowest estimated cost. Plan execution: Once the optimizer has selected the best execution plan, the query is executed using that plan. The DBMS retrieves the required data from the storage and applies the operations specified in the plan to produce the query result. There are several techniques used in query cost optimization. Some of these techniques include: Join ordering: The optimizer evaluates the different join orders for a query and selects the order that will minimize the cost of executing the query. Index selection: The optimizer determines which indexes to use to retrieve data from the storage to minimize the cost of accessing the data. Predicate pushdown: The optimizer pushes the selection predicates down to the lowest possible level in the query plan to reduce the amount of data that needs to be processed. Query rewriting: The optimizer rewrites the query in a more efficient form, such as converting a subquery to a join operation or simplifying complex expressions. By using these techniques, the query cost optimization process can significantly improve the performance of a database query, reducing the amount of time and resources required to execute the query while producing the correct results. ","date":"2023-04-09","objectID":"/queryoptimizationtechniques/:0:12","tags":["ChatGPT","Query optimization"],"title":"Query optimization","uri":"/queryoptimizationtechniques/"},{"categories":["ChatGPT Report"],"content":"You query cost estimation ","date":"2023-04-09","objectID":"/queryoptimizationtechniques/:0:13","tags":["ChatGPT","Query optimization"],"title":"Query optimization","uri":"/queryoptimizationtechniques/"},{"categories":["ChatGPT Report"],"content":"ChatGPT Query cost estimation is a process used by database query optimizers to estimate the cost of executing a query using different execution plans. The goal of query cost estimation is to determine which execution plan is the most efficient in terms of minimizing the resources required to execute the query. The cost of executing a query can be measured in terms of CPU time, disk I/O, network bandwidth, and other resources that are required to process the query. Query cost estimation involves estimating the cost of each execution plan based on these resources. There are several factors that are considered in query cost estimation, including: Join algorithms: Different join algorithms, such as nested loop join, hash join, and merge join, have different costs depending on the size and distribution of the data being joined. Data distribution: The distribution of data across tables and partitions can affect the cost of executing a query. For example, if data is highly skewed, certain execution plans may be more efficient than others. Indexing: The presence or absence of indexes can affect the cost of accessing data. The query optimizer needs to estimate the cost of using different indexes to retrieve data from the storage. Query selectivity: The selectivity of a query refers to the percentage of rows that meet the query’s conditions. The query optimizer needs to estimate the cost of filtering the data based on the selectivity of the query. Storage layout: The storage layout of the database can affect the cost of accessing data. For example, if data is stored in a columnar format, certain execution plans may be more efficient than others. To estimate the cost of executing a query, the query optimizer generates one or more execution plans and estimates the cost of executing each plan. The optimizer then selects the execution plan with the lowest estimated cost. Query cost estimation is an important part of the query optimization process, as it helps the query optimizer select the most efficient execution plan for a given query. By selecting the most efficient execution plan, the query optimizer can significantly improve the performance of the database and reduce the amount of resources required to execute queries. ","date":"2023-04-09","objectID":"/queryoptimizationtechniques/:0:14","tags":["ChatGPT","Query optimization"],"title":"Query optimization","uri":"/queryoptimizationtechniques/"},{"categories":["ChatGPT Report"],"content":"You query operations ","date":"2023-04-09","objectID":"/queryoptimizationtechniques/:0:15","tags":["ChatGPT","Query optimization"],"title":"Query optimization","uri":"/queryoptimizationtechniques/"},{"categories":["ChatGPT Report"],"content":"ChatGPT In the context of databases, a query is a request for data from one or more tables. Queries are executed using specific operations that manipulate the data and produce the desired result set. Here are some common query operations: SELECT: This is the most common query operation, used to retrieve data from one or more tables based on specific criteria. The SELECT operation can also be used to perform calculations, combine data from multiple tables, and format the output. FROM: This operation specifies the table or tables that the data will be retrieved from. The FROM operation can also include join operations to combine data from multiple tables. WHERE: This operation filters the data based on specific conditions. The WHERE operation can include logical operators (AND, OR, NOT) and comparison operators (\u003e, \u003c, =) to specify the conditions. GROUP BY: This operation groups the data by one or more columns, and then performs calculations on each group. For example, it can be used to calculate the average or sum of a specific column for each group. HAVING: This operation filters the grouped data based on specific conditions. The HAVING operation is similar to the WHERE operation, but it is applied to the grouped data rather than the individual rows. ORDER BY: This operation sorts the result set by one or more columns in ascending or descending order. LIMIT: This operation limits the number of rows returned by the query. It is often used in combination with the ORDER BY operation to retrieve the top or bottom rows based on a specific column. These query operations can be combined in different ways to produce complex queries that retrieve, manipulate, and analyze large amounts of data. The specific operations used in a query depend on the desired result and the structure of the data in the database. ","date":"2023-04-09","objectID":"/queryoptimizationtechniques/:0:16","tags":["ChatGPT","Query optimization"],"title":"Query optimization","uri":"/queryoptimizationtechniques/"},{"categories":["ChatGPT Report"],"content":"You selection operations ","date":"2023-04-09","objectID":"/queryoptimizationtechniques/:0:17","tags":["ChatGPT","Query optimization"],"title":"Query optimization","uri":"/queryoptimizationtechniques/"},{"categories":["ChatGPT Report"],"content":"ChatGPT In the context of databases, selection operations refer to the process of retrieving a subset of data from a table based on specific conditions. The selection operation is typically performed using the SQL SELECT statement with the WHERE clause. The WHERE clause allows you to specify one or more conditions that the data must meet in order to be selected. The following comparison operators can be used in the WHERE clause: = (equal to): selects rows where the specified column is equal to a given value. != or \u003c\u003e (not equal to): selects rows where the specified column is not equal to a given value. \u003c (less than): selects rows where the specified column is less than a given value. (greater than): selects rows where the specified column is greater than a given value. \u003c= (less than or equal to): selects rows where the specified column is less than or equal to a given value. = (greater than or equal to): selects rows where the specified column is greater than or equal to a given value. BETWEEN: selects rows where the specified column is between two given values. LIKE: selects rows where the specified column matches a given pattern using wildcards (% and _). IN: selects rows where the specified column matches any value in a list of given values. In addition to these comparison operators, the WHERE clause can also include logical operators such as AND, OR, and NOT to combine multiple conditions. Selection operations are essential for retrieving specific data from a large table based on specific criteria. By using selection operations, you can filter out irrelevant data and focus only on the data that meets your requirements. ","date":"2023-04-09","objectID":"/queryoptimizationtechniques/:0:18","tags":["ChatGPT","Query optimization"],"title":"Query optimization","uri":"/queryoptimizationtechniques/"},{"categories":["ChatGPT Report"],"content":"You A2 (binary search) A3 (primary index, equality on key) A4(secondary index, equality on non key) A5 A7 A10 ","date":"2023-04-09","objectID":"/queryoptimizationtechniques/:0:19","tags":["ChatGPT","Query optimization"],"title":"Query optimization","uri":"/queryoptimizationtechniques/"},{"categories":["ChatGPT Report"],"content":"ChatGPT A2. Binary Search: Binary search is a search algorithm that operates on a sorted collection of data, typically an array or a list. It works by repeatedly dividing the search interval in half until the target value is found or the search interval is empty. Binary search has a time complexity of O(log n), which makes it very efficient for large collections of data. For example, let’s say you have a sorted list of numbers [1, 3, 5, 7, 9, 11, 13] and you want to find the position of the number 7. You can use binary search to quickly find the position by repeatedly dividing the search interval in half. Here are the steps: Set the start index to 0 and the end index to the length of the list minus one. Calculate the middle index by adding the start and end indices and dividing by two. Compare the middle element to the target value (7 in this case). If the middle element is equal to the target value, return its position. If the middle element is less than the target value, set the start index to the middle index plus one. If the middle element is greater than the target value, set the end index to the middle index minus one. Repeat steps 2 to 6 until the target value is found or the search interval is empty. In this case, the algorithm would return the position 3, which is the index of the number 7 in the list. A3. Primary Index, Equality on Key: A primary index is an index that is created on a table’s primary key column(s) to improve the performance of queries that use equality operations on the primary key. The primary key is a column or combination of columns that uniquely identifies each row in the table. By creating an index on the primary key, the database can quickly locate the rows that match a specific value or set of values. For example, let’s say you have a table called “customers” with the following columns: customer_id, name, address, and phone_number. The customer_id column is the primary key of the table, and you want to retrieve the information for a specific customer with the ID 12345. By creating a primary index on the customer_id column, the database can quickly locate the row that matches the ID 12345 without having to scan the entire table. A4. Secondary Index, Equality on Non-Key: A secondary index is an index that is created on a non-key column(s) to improve the performance of queries that use equality operations on the non-key column. Unlike a primary index, a secondary index does not guarantee uniqueness, as multiple rows can have the same value in the indexed column(s). For example, let’s say you have a table called “products” with the following columns: product_id, name, category, and price. You want to retrieve the information for all products in the “electronics” category. By creating a secondary index on the category column, the database can quickly locate the rows that match the category “electronics” without having to scan the entire table. A5. Join: A join is an operation that combines rows from two or more tables based on a related column(s). The related column(s) are typically the primary and foreign keys of the tables. For example, let’s say you have two tables called “orders” and “customers”. The orders table has columns for order_id, customer_id, and order_date, while the customers table has columns for customer_id, name, and address. To retrieve the order information along with the customer name and address, you can use a join operation on the customer_id column: SELECT orders.order_id, orders.order_date, customers ","date":"2023-04-09","objectID":"/queryoptimizationtechniques/:0:20","tags":["ChatGPT","Query optimization"],"title":"Query optimization","uri":"/queryoptimizationtechniques/"},{"categories":["ChatGPT Report"],"content":"You Query Operation 5.3.1Slection operation  File scan – search algorithms that locate and retrieve records that fulfill a selection condition.  Algorithm A1 (linear search). Scan each file block and test all records to see whether they satisfy the selection condition.  Cost estimate = br block transfers + 1 seek ♦ br denotes number of blocks containing records from relation r  If selection is on a key attribute, can stop on finding record ♦ cost = (br /2) block transfers + 1 seek  Linear search can be applied regardless of ♦ selection condition or ♦ ordering of records in the file, or ♦ availability of indices  A2 (binary search). Applicable if selection is an equality comparison on the attribute on which file is ordered. Bal Krishna Nyaupane Page 3         Assume that the blocks of a relation are stored contiguously Cost estimate (number of disk blocks to be scanned): ♦ cost of locating the first tuple by a binary search on the blocks log2(br) * (tT + tS) ♦ If there are multiple records satisfying selection • Add transfer cost of the number of blocks containing records that satisfy selection condition • Will see how to estimate this cost in Chapter 14 Index scan – search algorithms that use an index  Selection condition must be on search-key of index. A3 (primary index on candidate key, equality). Retrieve a single record that satisfies the corresponding equality condition  Cost = (hi + 1) * (tT + tS ) A4 (primary index on nonkey, equality) Retrieve multiple records.  Records will be on consecutive blocks ♦ Let b = number of blocks containing matching records  = hi * (tT + tS) + tS + tT * b A5 (equality on search-key of secondary index).  Retrieve a single record if the search-key is a candidate key ♦ Cost = (hi + 1) * (tT + tS)  Retrieve multiple records if search-key is not a candidate key ♦ each of n matching records may be on a different block ♦ Cost = (hi + n) * (tT + tS) • Can be very expensive!  Can implement selections of the form σA≤V (r) or σA ≥ V(r) by using ♦ a linear file scan or binary search, ♦ or by using indices in the following ways: A6 (primary index, comparison). (Relation is sorted on A)  For σA ≥ V(r) use index to find first tuple ≥ v and scan relation sequentially from there  For σ A≤V (r) just scan relation sequentially till first tuple \u003e v; do not use index A7 (secondary index, comparison).  For σA ≥ V(r) use index to find first index entry ≥ v and scan index sequentially from there, to find pointers to records.  For σA≤V (r) just scan leaf pages of index finding pointers to records, till first entry \u003e v  In either case, retrieve records that are pointed to ♦ requires an I/O for each record ♦ Linear file scan may be cheaper ","date":"2023-04-09","objectID":"/queryoptimizationtechniques/:0:21","tags":["ChatGPT","Query optimization"],"title":"Query optimization","uri":"/queryoptimizationtechniques/"},{"categories":["ChatGPT Report"],"content":"ChatGPT A2 (binary search): This algorithm is used when the selection operation involves an equality comparison on an attribute on which the file is ordered. The cost estimate for this algorithm is based on the number of disk blocks to be scanned. Assuming that the blocks of a relation are stored contiguously, the cost estimate can be calculated as (log2(br)) * (tT + tS), where br is the number of blocks containing records from relation r, tT is the time required to transfer one block from disk to memory, and tS is the time required to perform a seek operation. A3 (primary index on candidate key, equality): This algorithm retrieves a single record that satisfies the corresponding equality condition. The cost estimate for this algorithm is (hi + 1) * (tT + tS), where hi is the height of the B+ tree index. A4 (primary index on non-key, equality): This algorithm retrieves multiple records that satisfy the corresponding equality condition. The records will be on consecutive blocks, and the cost estimate is hi * (tT + tS) + tS + tT * b, where b is the number of blocks containing matching records. A5 (equality on search-key of secondary index): This algorithm retrieves a single record if the search-key is a candidate key, and the cost estimate is (hi + 1) * (tT + tS). If the search-key is not a candidate key, each of the n matching records may be on a different block, and the cost estimate is (hi + n) * (tT + tS), which can be very expensive. A7 (secondary index, comparison): This algorithm is used when the selection operation involves a comparison and a secondary index is available. For σA ≥ V(r), the algorithm uses the index to find the first index entry ≥ v and scans the index sequentially from there to find pointers to records. For σA ≤ V(r), the algorithm scans the leaf pages of the index finding pointers to records, till the first entry \u003e v. In either case, the algorithm retrieves records that are pointed to, which requires an I/O for each record. A linear file scan may be cheaper. ","date":"2023-04-09","objectID":"/queryoptimizationtechniques/:0:22","tags":["ChatGPT","Query optimization"],"title":"Query optimization","uri":"/queryoptimizationtechniques/"},{"categories":["ChatGPT Report"],"content":"You sorting and join operation ","date":"2023-04-09","objectID":"/queryoptimizationtechniques/:0:23","tags":["ChatGPT","Query optimization"],"title":"Query optimization","uri":"/queryoptimizationtechniques/"},{"categories":["ChatGPT Report"],"content":"ChatGPT Sorting and join operations are fundamental operations in database systems that are used to combine data from two or more tables. Here is an overview of these operations: Sorting Operation: Sorting operation sorts the tuples in a relation based on one or more attributes. It is used to arrange data in a specific order that satisfies the requirement of a particular query. Sorting can be performed either in-memory or on-disk. When sorting is performed on-disk, it requires the creation of a temporary file to store the sorted data. The cost of sorting depends on the size of the relation and the memory available for sorting. The most common algorithm used for sorting is the external merge sort, which works by dividing the data into smaller chunks that fit into memory, sorting them, and merging them to produce the final sorted output. Join Operation: Join operation is used to combine two or more tables into a single table based on a common attribute. There are several types of join operations, including: Inner Join: It returns only the matching tuples from both tables based on a specified join condition. Outer Join: It returns all tuples from one table and matching tuples from the other table(s) based on a specified join condition. There are three types of outer join: left outer join, right outer join, and full outer join. Cross Join: It returns all possible combinations of tuples from both tables. The cost of a join operation depends on the size of the tables, the complexity of the join condition, and the available memory for processing. Join algorithms include nested loop join, sort-merge join, hash join, and index join. The most appropriate algorithm is chosen based on the characteristics of the tables being joined. In summary, sorting and join operations are essential components of database processing. They allow users to combine data from multiple tables and organize it in a meaningful way for analysis and decision-making. ","date":"2023-04-09","objectID":"/queryoptimizationtechniques/:0:24","tags":["ChatGPT","Query optimization"],"title":"Query optimization","uri":"/queryoptimizationtechniques/"},{"categories":["ChatGPT Report"],"content":"You describe puter join and equi join ","date":"2023-04-09","objectID":"/queryoptimizationtechniques/:0:25","tags":["ChatGPT","Query optimization"],"title":"Query optimization","uri":"/queryoptimizationtechniques/"},{"categories":["ChatGPT Report"],"content":"ChatGPT In database management systems, join operations combine data from two or more tables based on a common column. There are several types of join operations, including nested loop join, sort-merge join, hash join, and index join. Two common types of joins are the cartesian product (or cross join) and the equi-join. A cartesian product, also known as a cross join, returns all possible combinations of rows from two tables. In a cartesian product, each row from the first table is paired with every row from the second table. For example, if table A has 3 rows and table B has 4 rows, a cartesian product of A and B would have 12 rows. On the other hand, an equi-join is a type of join that combines rows from two tables where the values in a specified column are equal. An equi-join can be performed using different algorithms, including nested loop join, sort-merge join, hash join, and index join. In an equi-join, the columns used for comparison are called join columns, and they must have the same name and data type in both tables. Here’s an example of an equi-join: Table A id name 1 Alice 2 Bob 3 Charlie Table B id salary 2 50000 3 60000 4 70000 Equi-join on id: SELECT A.name, B.salary FROM A JOIN B ON A.id = B.id; Result: name salary Bob 50000 Charlie 60000 In this example, the equi-join returns only the rows where the id column matches in both tables, and the resulting table includes the name column from table A and the salary column from table B. ","date":"2023-04-09","objectID":"/queryoptimizationtechniques/:0:26","tags":["ChatGPT","Query optimization"],"title":"Query optimization","uri":"/queryoptimizationtechniques/"},{"categories":["ChatGPT Report"],"content":"You inner join and its types ","date":"2023-04-09","objectID":"/queryoptimizationtechniques/:0:27","tags":["ChatGPT","Query optimization"],"title":"Query optimization","uri":"/queryoptimizationtechniques/"},{"categories":["ChatGPT Report"],"content":"ChatGPT Inner join, also known as equi-join, is a type of join operation in relational databases that combines rows from two or more tables based on a related column between them. The related column is usually a primary key or foreign key in the tables. There are three main types of inner joins: Inner join: Also known as equi-join, this type of join returns only the rows from both tables that have matching values in the specified column. Self join: This type of join is used when a table is joined with itself. It is used to compare rows within a single table based on a related column. Natural join: This type of join is used when there is a common column with the same name in both tables. The join will automatically match the rows with the same value in the common column. Inner join is useful when you need to combine information from multiple tables that have related data. It allows you to create a single table that contains all the information you need for a particular query or report. ","date":"2023-04-09","objectID":"/queryoptimizationtechniques/:0:28","tags":["ChatGPT","Query optimization"],"title":"Query optimization","uri":"/queryoptimizationtechniques/"},{"categories":["ChatGPT Report"],"content":"You outer join and its type ","date":"2023-04-09","objectID":"/queryoptimizationtechniques/:0:29","tags":["ChatGPT","Query optimization"],"title":"Query optimization","uri":"/queryoptimizationtechniques/"},{"categories":["ChatGPT Report"],"content":"ChatGPT Outer join is a type of join operation in relational databases that includes all the rows from one table and the matching rows from another table. If there is no matching row in the second table, then the resulting table will contain NULL values in the columns of the second table. There are three types of outer joins: Left outer join (or left join): Includes all the rows from the left table and the matching rows from the right table. If there is no matching row in the right table, the result will contain NULL values in the columns of the right table. For example, consider two tables, customers and orders. A left outer join of these tables would include all the customers, and the orders that each customer has placed (if any). If a customer has not placed any orders, the corresponding columns for orders will contain NULL values. Right outer join (or right join): Includes all the rows from the right table and the matching rows from the left table. If there is no matching row in the left table, the result will contain NULL values in the columns of the left table. Using the same example, a right outer join of customers and orders would include all the orders, and the customers who placed each order (if any). If an order does not have a corresponding customer, the corresponding columns for customers will contain NULL values. Full outer join (or full join): Includes all the rows from both tables. If there is no matching row in either table, the result will contain NULL values in the columns of the table where there is no matching row. In the example of customers and orders, a full outer join would include all the customers and all the orders, and match them where possible. If a customer has not placed any orders, the corresponding columns for orders will contain NULL values, and vice versa. If a customer and an order do not match, the corresponding columns for the other table will contain NULL values. ","date":"2023-04-09","objectID":"/queryoptimizationtechniques/:0:30","tags":["ChatGPT","Query optimization"],"title":"Query optimization","uri":"/queryoptimizationtechniques/"},{"categories":["ChatGPT Report"],"content":"You evaluation of the expression ","date":"2023-04-09","objectID":"/queryoptimizationtechniques/:0:31","tags":["ChatGPT","Query optimization"],"title":"Query optimization","uri":"/queryoptimizationtechniques/"},{"categories":["ChatGPT Report"],"content":"ChatGPT In computer programming, the evaluation of an expression refers to the process of computing the value of the expression using the given input values and following the operator precedence rules. For example, consider the expression “3 + 4 * 5 - 6”. Following the operator precedence rules, we first evaluate the multiplication operation (4*5=20), then add the result to 3 (3+20=23), and finally subtract 6 to get the final result of 17. The evaluation of an expression can involve different types of operators, such as arithmetic, comparison, logical, and bitwise operators. The order in which the operators are evaluated depends on their precedence and associativity. For instance, in the expression “a + b * c \u003c= d || e \u0026\u0026 f”, the multiplication operation has higher precedence than addition, so we first evaluate “b * c”. The comparison operator “\u003c=” has higher precedence than the logical OR “||”, so we evaluate “a + (b * c) \u003c= d” before checking the OR condition. Finally, we evaluate the AND condition “e \u0026\u0026 f” and check the logical OR between the two sub-expressions. Overall, the evaluation of expressions is a fundamental operation in programming and is used extensively in various contexts, such as control flow statements, function calls, and data manipulation operations. ","date":"2023-04-09","objectID":"/queryoptimizationtechniques/:0:32","tags":["ChatGPT","Query optimization"],"title":"Query optimization","uri":"/queryoptimizationtechniques/"},{"categories":["ChatGPT Report"],"content":"You materialization and pipelining ","date":"2023-04-09","objectID":"/queryoptimizationtechniques/:0:33","tags":["ChatGPT","Query optimization"],"title":"Query optimization","uri":"/queryoptimizationtechniques/"},{"categories":["ChatGPT Report"],"content":"ChatGPT Materialization and pipelining are two different strategies used in query processing and optimization. Materialization refers to the process of storing intermediate results of a query as temporary tables in the database. These temporary tables can then be used as inputs for subsequent operations in the query. Materialization can improve query performance by reducing the number of times data needs to be accessed from the original tables. However, it can also increase the amount of disk space used by the database, and slow down queries if the temporary tables are very large or the disk I/O is slow. Pipelining, on the other hand, is a strategy that avoids the use of temporary tables altogether by passing intermediate results directly from one operation to the next. Pipelining can improve query performance by reducing the overhead associated with materializing and reading from temporary tables. However, pipelining can be more complex to implement and optimize, and may not always be possible depending on the structure of the query. In general, materialization is more suitable for complex queries involving multiple operations or large amounts of data, while pipelining is more suitable for simple queries or queries with smaller data sets. The choice between materialization and pipelining ultimately depends on the specific query and the characteristics of the database system being used. ","date":"2023-04-09","objectID":"/queryoptimizationtechniques/:0:34","tags":["ChatGPT","Query optimization"],"title":"Query optimization","uri":"/queryoptimizationtechniques/"},{"categories":["ChatGPT Report"],"content":"You cost based optimization(physical) and Heuristics optimizations(logical) in details ","date":"2023-04-09","objectID":"/queryoptimizationtechniques/:0:35","tags":["ChatGPT","Query optimization"],"title":"Query optimization","uri":"/queryoptimizationtechniques/"},{"categories":["ChatGPT Report"],"content":"ChatGPT In query optimization, there are two main approaches: cost-based optimization and heuristic optimization. Cost-based optimization involves estimating the cost of executing different plans for a query and selecting the one with the lowest cost. The cost of a plan is estimated by considering factors such as the number of disk accesses, CPU time, and memory usage required to execute the plan. The optimizer uses statistics about the database such as the number of tuples in each relation, the distribution of values in each attribute, and the size of each relation to estimate the cost of each plan. Cost-based optimization is generally considered to be more effective than heuristic optimization because it takes into account the specifics of the database and the query being executed. The optimizer can evaluate many possible plans and choose the best one, even if it is not obvious which plan is best based on the structure of the query. Heuristic optimization, on the other hand, involves applying rules of thumb to generate a good plan for a query. This approach is often used when there are no statistics available about the database, or when the optimizer cannot evaluate all possible plans due to resource constraints. There are various techniques that can be used for both cost-based and heuristic optimization, some of which are listed below: Cost-Based Optimization: a. Exhaustive Search: In this technique, the optimizer evaluates all possible plans for a query and selects the one with the lowest estimated cost. This technique is not always feasible due to the large number of possible plans. b. Dynamic Programming: This technique uses a table to store the best plan for each subquery and then combines these plans to generate the best plan for the entire query. c. Heuristic Search: In this technique, the optimizer starts with an initial plan and uses heuristics to generate new plans. The optimizer selects the best plan found so far and continues generating new plans until it converges on the best plan. Heuristic Optimization: a. Greedy Algorithm: This algorithm starts with an initial plan and repeatedly applies local optimizations to improve the plan until no further improvements are possible. b. Hill Climbing: This algorithm starts with an initial plan and generates a sequence of plans by making small modifications to the current plan. The optimizer selects the plan with the lowest cost and continues generating new plans until it converges on the best plan. c. Genetic Algorithms: This approach involves generating a population of plans and using techniques inspired by genetics to evolve new plans that are better than the previous generation. In summary, cost-based optimization and heuristic optimization are two approaches to query optimization, and both have their advantages and disadvantages. Cost-based optimization is generally more effective but can be computationally expensive, while heuristic optimization can be faster but may not always find the best plan. ","date":"2023-04-09","objectID":"/queryoptimizationtechniques/:0:36","tags":["ChatGPT","Query optimization"],"title":"Query optimization","uri":"/queryoptimizationtechniques/"},{"categories":["ChatGPT Report"],"content":"You query decomposition ","date":"2023-04-09","objectID":"/queryoptimizationtechniques/:0:37","tags":["ChatGPT","Query optimization"],"title":"Query optimization","uri":"/queryoptimizationtechniques/"},{"categories":["ChatGPT Report"],"content":"ChatGPT Query decomposition is the process of breaking down a complex query into smaller, simpler sub-queries that can be executed more efficiently. The aim of query decomposition is to improve query performance and reduce the overall query response time. There are two main approaches to query decomposition: horizontal and vertical decomposition. Horizontal decomposition involves splitting the query into sub-queries that operate on different subsets of the data. For example, a query that involves selecting data from two tables can be decomposed into two sub-queries: one that selects data from the first table and another that selects data from the second table. These sub-queries can then be executed in parallel, which can reduce the overall query response time. Vertical decomposition, on the other hand, involves breaking down the query into sub-queries that operate on different attributes of the data. For example, a query that involves selecting data from multiple columns of a table can be decomposed into multiple sub-queries, each of which selects data from a single column. These sub-queries can then be executed in parallel, which can also reduce the overall query response time. Query decomposition can be performed automatically by the query optimizer, which is a component of a database management system that is responsible for analyzing queries and determining the most efficient execution plan. The optimizer uses various techniques, such as cost-based optimization and heuristic optimization, to generate the execution plan that is most likely to produce the best performance. Overall, query decomposition is an important technique for improving query performance and reducing query response time in large, complex database systems. By breaking down complex queries into smaller sub-queries that can be executed more efficiently, query decomposition enables databases to process queries more quickly and with greater accuracy. ","date":"2023-04-09","objectID":"/queryoptimizationtechniques/:0:38","tags":["ChatGPT","Query optimization"],"title":"Query optimization","uri":"/queryoptimizationtechniques/"},{"categories":["ChatGPT Report"],"content":"You distributed query processing step ","date":"2023-04-09","objectID":"/queryoptimizationtechniques/:0:39","tags":["ChatGPT","Query optimization"],"title":"Query optimization","uri":"/queryoptimizationtechniques/"},{"categories":["ChatGPT Report"],"content":"ChatGPT Distributed query processing refers to the process of executing a single query on multiple distributed databases or nodes in a distributed computing environment. The basic steps involved in distributed query processing are as follows: Query Decomposition: The first step in distributed query processing is to break down the original query into smaller sub-queries that can be executed on individual nodes or databases. This is typically done by a query optimizer or query planner. Data Localization: The second step is to identify which data needs to be accessed by each sub-query and where that data is located. This involves determining the location of the required data and deciding which nodes or databases should be used to access that data. Data Retrieval: The third step is to retrieve the required data from the appropriate nodes or databases. This involves sending sub-queries to the appropriate nodes, retrieving the results, and combining the results to produce the final result set. Result Consolidation: The fourth and final step is to consolidate the results of the sub-queries into a single result set. This involves merging the results from each sub-query into a final result set, which is then returned to the user. The process of distributed query processing can be complex and requires careful consideration of factors such as network bandwidth, data transfer costs, and load balancing. As a result, a number of techniques have been developed to optimize distributed query processing, including parallel query processing, data partitioning, and caching. ","date":"2023-04-09","objectID":"/queryoptimizationtechniques/:0:40","tags":["ChatGPT","Query optimization"],"title":"Query optimization","uri":"/queryoptimizationtechniques/"},{"categories":["ChatGPT Report"],"content":"You steps of query decomposition ","date":"2023-04-09","objectID":"/queryoptimizationtechniques/:0:41","tags":["ChatGPT","Query optimization"],"title":"Query optimization","uri":"/queryoptimizationtechniques/"},{"categories":["ChatGPT Report"],"content":"ChatGPT Query decomposition is the process of breaking down a complex query into smaller sub-queries that can be executed by distributed databases in a distributed computing environment. The following are the general steps involved in query decomposition: Query Parsing: In this step, the query is parsed to extract its syntactic structure and to identify the various components such as tables, columns, conditions, and operators. Query Analysis: In this step, the query is analyzed to identify any potential issues such as data skew, data locality, network latency, and resource availability. Query Optimization: In this step, various optimization techniques such as cost-based optimization or heuristic-based optimization are applied to optimize the execution plan of the query. The optimized query plan should take into account the data distribution, network bandwidth, and available resources of the distributed computing environment. Query Decomposition: In this step, the query is decomposed into smaller sub-queries that can be executed by the distributed databases. The decomposition should be done in a way that minimizes the data transfer between the distributed databases and maximizes the parallelism of the execution. Query Distribution: In this step, the sub-queries are distributed to the appropriate distributed databases. The sub-queries may be sent to different databases based on the data distribution and the processing power of the databases. Query Execution: In this step, the sub-queries are executed in parallel by the distributed databases. The results are then combined using a join operation to produce the final result. Result Aggregation: In this step, the results from each distributed database are aggregated to produce the final result of the query. Result Delivery: In this step, the final result of the query is delivered to the user or application that initiated the query. Overall, query decomposition is a complex process that involves several steps to ensure that the query is executed efficiently in a distributed computing environment. ","date":"2023-04-09","objectID":"/queryoptimizationtechniques/:0:42","tags":["ChatGPT","Query optimization"],"title":"Query optimization","uri":"/queryoptimizationtechniques/"},{"categories":["ChatGPT Report"],"content":"You query decomposition steps: Normalization Analysis ELimination rewriting ","date":"2023-04-09","objectID":"/queryoptimizationtechniques/:0:43","tags":["ChatGPT","Query optimization"],"title":"Query optimization","uri":"/queryoptimizationtechniques/"},{"categories":["ChatGPT Report"],"content":"ChatGPT Query decomposition is the process of breaking down a complex query into smaller, more manageable subqueries that can be executed efficiently. The steps involved in query decomposition are: Normalization: In this step, the query is transformed into a standard format that can be easily analyzed and optimized. This involves breaking the query down into its constituent parts, such as tables, columns, and operators. Analysis: Once the query has been normalized, it is analyzed to identify any potential performance bottlenecks. This involves analyzing the query execution plan, which outlines the steps that the database must take to execute the query. Elimination: In this step, any unnecessary or redundant subqueries are eliminated to simplify the overall query and reduce the processing time. Rewriting: In the final step, the query is rewritten as a set of smaller, more efficient subqueries that can be executed independently. This involves breaking down the query into smaller pieces, each of which can be optimized independently. Overall, the goal of query decomposition is to improve query performance by breaking down complex queries into smaller, more manageable subqueries that can be executed more efficiently. ","date":"2023-04-09","objectID":"/queryoptimizationtechniques/:0:44","tags":["ChatGPT","Query optimization"],"title":"Query optimization","uri":"/queryoptimizationtechniques/"},{"categories":["ChatGPT Report"],"content":"You Elimination of Redundancy ","date":"2023-04-09","objectID":"/queryoptimizationtechniques/:0:45","tags":["ChatGPT","Query optimization"],"title":"Query optimization","uri":"/queryoptimizationtechniques/"},{"categories":["ChatGPT Report"],"content":"ChatGPT Elimination of redundancy is an important step in query decomposition. It involves removing unnecessary or redundant computations from the query plan to improve its efficiency. Redundancy can occur when the same computation is performed multiple times or when the same data is accessed repeatedly. The elimination of redundancy involves the following steps: Common subexpression elimination: This step involves identifying and eliminating computations that are repeated multiple times in the query plan. The result of a computation is stored in a temporary table, and the table is referenced in subsequent computations instead of recomputing the result each time. Join elimination: This step involves eliminating unnecessary join operations. A join operation can be eliminated if one of the relations involved in the join is not used in the query. Predicate pushdown: This step involves pushing down predicates in the query plan to reduce the amount of data that needs to be processed. Predicates that can be evaluated at a lower level of the query plan are pushed down to that level. Projection elimination: This step involves eliminating unnecessary projection operations. A projection operation can be eliminated if the attributes that are projected are not used in subsequent operations in the query plan. Deadlock elimination: This step involves removing deadlock situations in the query plan. Deadlocks occur when two or more operations are waiting for each other to complete. The deadlock can be eliminated by reordering the operations in the query plan. The elimination of redundancy helps to reduce the number of computations and the amount of data that needs to be processed, which in turn improves the efficiency of the query plan. ","date":"2023-04-09","objectID":"/queryoptimizationtechniques/:0:46","tags":["ChatGPT","Query optimization"],"title":"Query optimization","uri":"/queryoptimizationtechniques/"},{"categories":["AI Project Report"],"content":" TRIBHUVAN UNIVERSITY INSTITUTE OF ENGINEERING PULCHOWK CAMPUS Machine Learning Analysis of Tirhuta Lipi By: Sumit Yadav (076/BCT/088) Raju Kumar Yadav (076/BCT/100) Prashant Bhandari (076/BCT/049) AN AI PROJECT REPORT TO THE DEPARTMENT OF ELECTRONICS AND COMPUTER ENGINEERING. DEPARTMENT OF ELECTRONICS AND COMPUTER ENGINEERING LALITPUR, NEPAL March, 2023 ii ACKNOWLEDGEMENT This project is prepared in partial fulfilment of the requirement for for the the bachelor’s degree in Electronics and Communication Engineering. First and foremost, We would also like to extend our sincere thanks to our passout seniors, friends, and family for their support and guidance throughout our research project. Their valuable inputs and feedback have been instrumental in shaping our ideas and improving our work. We would also like to express our gratitude to the Department of Electronics and Computer Engineering, Pulchowk Campus, Tribhuvan University for providing us with the necessary resources and infrastructure to carry out this research. Last but not least, we would like to thank our instructor, Basant Joshi, for his guidance, encouragement, and support throughout this project. His expertise and knowledge have been invaluable in helping us to achieve our research objectives. Finally, we would like to acknowledge the contribution of all the authors of this research report, namely Sumit Yadav, Raju Kumar Yadav, and Prashant Bhandari, for their hard work, dedication, and collaborative efforts in producing this research. Any kind of suggestion or criticism will be highly appreciated and acknowledged. iii ABSTRACT Tirhuta Lipi is a low resource language that is primarily spoken in the Indian states of Bi- har and Terai Belt of Nepal. The script is not widely used, and there are limited resources available for language processing tasks. In this project, we have explored machine learning techniques for character recognition of the low-resource language Tirhuta Lipi. We col- lected a diverse dataset of Tirhuta vowels Lipi and preprocessed it for training machine learning models. We tested different algorithms such as Sklearn and Tensorflow Keras to evaluate their performance in character recognition. Our results showed that the mobile net embedding with logistic regression achieved the highest accuracy of 0.97. We also found that fine-tuning the mobile net model in Tensorflow Keras resulted in an accuracy of 0.965. Our study demonstrates the feasibility of using machine learning for character recognition in TirhutaLipi,whichcouldhaveimportantapplicationsinareassuchastexttranslationandop- tical character recognition. This project contributes to the growing field of natural language processing for low-resource languages and highlights the potential of machine learning in language-related tasks. Keywords: Tirhuta Lipi, Character recognition, Digital accessibility iv TABLE OF CONTENTS TITLE PAGE i ACKNOWLEDGEMENT ii ABSTRACT iii TABLE OF CONTENTS v LIST OF FIGURES vi 1 INTRODUCTION 1 Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 Objectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 Problem statement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 Scope of Project . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 2 LITERATURE REVIEW 4 3 THEORETICAL BACKGROUND 5 4 METHODOLOGY 6 Data Collection Process . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 Preprocessing and Cleaning of Data . . . . . . . . . . . . . . . . . . . . . 7 Data Analysis and Visualization Techniques . . . . . . . . . . . . . . . . . 8 Machine Learning Algorithms Used . . . . . . . . . . . . . . . . . . . . . 8 4.4.1 Decision Tree Classification . . . . . . . . . . . . . . . . . . . . . 9 Mobile Net Embedding . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 Fine Tuning MobileNet . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 5 TOOLS AND TECHNOLOGIES 12 6 RESULTS 13 De","date":"2023-04-01","objectID":"/ioe_bachelors_ai_project_pulchowk_campus/:0:0","tags":["Machile Learning","Tirhuta Lipi"],"title":"Mathili_Script","uri":"/ioe_bachelors_ai_project_pulchowk_campus/"},{"categories":null,"content":" Here in this post you will find different festivals, that are celebrated in the region of Mithila (side of Nepal janakpur, tirhut, and parts of Bihar). Mithila festivals is very much hidden from rest of the worlds. As mentioned in ramayan, every day in mithila is a day of new festival and tradition. They are Lets go with day in weeks. Sunday —\u003e Raib Fasting Monday —\u003e Sombarii Tuesday —\u003e Mangal Bari Thursday —\u003e Bairaspat Bari Friday —\u003e Sukra Bari Saturday —\u003e Jai Hanuman Sunday. In this day people celebrate god Vishnu. This have many varient according to month basic and year basics, This is also classifed according to What you eat during the fasting period they are eating without salt and eating nothing. Monday In this day people worship lord Shiva and Maa Parvati, by ofering Gud and Chaana. This have also many varient, They are 16 sombari(Doing only conscutive 16 sombari of each week), 6 month and 1.25 years etc, according to desire. ","date":"2022-11-28","objectID":"/mithila_festivals/:0:0","tags":null,"title":"Mithila_festivals","uri":"/mithila_festivals/"},{"categories":["technical Report"],"content":" Sumit Yadav (PUL076BCT088) Prof. Dr.  Subarna Shakya  Director, Information Technology Innovation Center, Tribhuvan University Computer Organization and Architecture 09 June 2022 TPUs (Tensor Processing Unit) Tensor Processing Units (TPUs) are custom-designed application-specific integrated circuits (ASICs) developed by Google to accelerate AI  and neural network works. Formally, it was begun in 2015 and first made available publicly  in 2018 for both cloud computing and small business. Tensor processing unit was first introduced in 2016 at Google I/O, when the company said the TPU had been already used in the company for over a year. This chip was quite specific to machine learning framework like TensorFlow, PyTorch and JAX. TPU is another kind of processing unit like a CPU or a GPU. There are, however, some big differences between those - the biggest difference being that TPU is an ASIC. And you probably know CPU and GPU are not, as they are not optimized to do one specific kind of application. CPU, GPU and TPUNow let's compare the difference between CPU, GPU and TPU. (For the case of multiple addition) A CPU performs the multiply-add operation by reading each input and weight from memory, multiplying them with its ALU (Arithmetic Logic Unit), writing them back to memory and finally adding up all the multiplies values. Modern CPUs are strengthened by massive cache, branch prediction and high clock rate on each of its cores. Which all contribute to a lower latency of the CPU.A GPU does however not use the fancy features which lower the latency. It also needs to orchestrate its thousands of ALUs which further decreases the latency. In short, GPUs drastically increase its throughput by parallelizing its computation in exchange for an increase in its latency.A TPU on the other hand operates very differently. Its ALUs are directly connected to each other without using the memory. They can directly give pass information, which will drastically decrease latency.                    Table 1. Benchmarked servers use Haswell CPUs, K80 GPUs, and TPUs. Haswell has 18 cores, and the K80 has 13 SMX processors. Now on the basics of workload (While training AI models):- CPUsQuick prototyping that requires maximum flexibilitySimple models that do not take long to trainModels that are dominated by custom TensorFlow operation written in C++Models that are  limited by available I/O or the networking bandwidth of the host machine.GPUsModels for which source does not exist or is too onerous to changeModels with a significant number of custom TensorFlow operations that must run at least partially on CPUsModels with TensorFlow ops that are not available on Cloud TPUMedium-to-large models with larger effective batch sizesTPUsModels dominated by matrix computationModels with no custom TensorFlow operations inside the main training loopModels that train for weeks or monthsLarger and very large models with very larger effective batch sizesTPU Origin, Architecture, and ImplementationStarting as early as 2006, we discussed deploying GPUs, FPGAs, or custom ASICs in our datacenters. We concluded that the few applications that could run on special hardware could be done virtually for free using the excess capacity of our large datacenters, and it’s hard to improve on free. The conversation changed in 2013 when a projection where people use voice search for 3 minutes a day using speech recognition DNNs would require our datacenters to double to meet computation demands, which would be very expensive to satisfy with conventional CPUs. Thus, we started a high-priority project to quickly produce a custom ASIC for inference (and bought off-the-shelf GPUs for training). The goal was to improve cost-performance by 10X over GPUs. Given this mandate, the TPU was designed, verified, built, and deployed in datacenters in just 15 months. Rather than be tightly integrated with a CPU, to reduce the chances of delaying deployment, the TPU was designed to be a coprocessor ","date":"2022-06-09","objectID":"/tensor_processing_units/:0:0","tags":["TPUs","Tensor_processing_units"],"title":"Tensor_processing_units","uri":"/tensor_processing_units/"},{"categories":["machine learning"],"content":"Multi_Class_Text_Classification This post and code are used in a challenge organized by LOCUS called DataRush. Hi!, Today in this post i am going to share our effort which was applied by us in a competition organized by LOCUS. Lets start, for this challenge we have give data sets which contain a train data csv which contain four columns and 390603 rows ie 390603 rows × 4 columns. As in each row we have given ‘id’ number( which is random id number to represent id), ‘abstract’ which contain main part for this competition which we have to classify.(Abstract :: It is the initial paragraph of the research paper which used to define the character and overview of research paper), ‘category’ this contain the category on which we have to classify the abstract data. and last column contain ‘category_num’ of respective category. df_train id abstract category category_num 0 271675 Bacteria are often exposed to multiple stimu... q-bio-QM 138 1 412276 Accurate knowledge of the thermodynamic prop... hep-ph- 68 2 256956 The largest X9.3 solar flare in solar cycle ... astro-ph-SR 7 3 427612 We say that a random integer variable $X$ is... math-PR 93 4 113852 We derive a formula expressing the joint dis... math-CO 76 ... ... ... ... ... 390598 479582 Axion-like particles (ALPs) are hypothetical... hep-ph- 68 390599 99488 Due to ever increasing usage of wireless dev... eess-SP 62 390600 157301 Weight and activation binarization is an eff... cs-CV 25 390601 209221 Large-scale unconstrained optimization is a ... math-OC 92 390602 482651 In this report, we discuss the dynamics of p... gr-qc- 65 390603 rows × 4 columns ","date":"2022-03-30","objectID":"/text_classification/:0:0","tags":["project","NLP"],"title":"Multi_Class_Text_Classification","uri":"/text_classification/"},{"categories":["machine learning"],"content":"Data visualization From EDA we have found that this data sets is so imblance. Some of the sample are one in number then other are of thousand in number. This add so complex in data classificatio. See what we found on category:- plt.figure(figsize=(25,4)) df_train['category'].value_counts().plot(kind='bar') plt.axis() plt.show() and similar things we found for test data. as all things in image we are unable to figure out how differ they are the we amnually plot number of data for each classes, then we socked from the results. We found two dat sample are of only one category, Shit. catetr=df_train['category'].value_counts() catetr cs-LG 23414 cs-CV 22943 quant-ph- 14561 cs-CL 11143 hep-ph- 10863 ... q-fin-EC 17 astro-ph- 7 q-alg- 2 alg-geom- 1 funct-an- 1 Name: category, Length: 156, dtype: int64 ","date":"2022-03-30","objectID":"/text_classification/:1:0","tags":["project","NLP"],"title":"Multi_Class_Text_Classification","uri":"/text_classification/"},{"categories":["machine learning"],"content":"Data Processing Here we have done stemmerming which makes shorten the word by removing the suffices and other letter which are added in the last of the sentence. Instead of lemmatization we have done stremmerming because stremmerming reduce the size of the words and preserve the manner and inner logics of the sentence. Here, we have used porterStemmer of NLTK library. ALong with that we have also removed some of the regular expression, puncutation, letter and replacing the comma by next line and we have also removed some of the stops word which is in this sentence, by making all sentence letter in lower case.That is passed through all data train test and validation data. # for stemmer ps = PorterStemmer() # function for data cleaning like regular expression punctuation lowering all to lower case removing stop words etc def clean_abstract(text): text = re.sub('[^a-z\\s]', ' ', text.lower()) text = [i.lower() for i in text.split() if i not in nlp.Defaults.stop_words] text = [ps.stem(i) for i in text] text = ' '.join(text) text.replace('\\n',' ') return text # cleaning train data df_train['abstract'] = df_train['abstract'].apply(clean_abstract) # cleaning test data df_test['abstract'] = df_test['abstract'].apply(clean_abstract) # cleaning val data df_val['abstract'] = df_val['abstract'].apply(clean_abstract) X_train, y_train = df_train['abstract'],df_train['category'] X_val, y_val = df_val['abstract'],df_val['category'] X_test= df_test['abstract'] ","date":"2022-03-30","objectID":"/text_classification/:2:0","tags":["project","NLP"],"title":"Multi_Class_Text_Classification","uri":"/text_classification/"},{"categories":["machine learning"],"content":"Text Vectorization ","date":"2022-03-30","objectID":"/text_classification/:3:0","tags":["project","NLP"],"title":"Multi_Class_Text_Classification","uri":"/text_classification/"},{"categories":["machine learning"],"content":"TFID ","date":"2022-03-30","objectID":"/text_classification/:3:1","tags":["project","NLP"],"title":"Multi_Class_Text_Classification","uri":"/text_classification/"},{"categories":["machine learning"],"content":"CountVectorizer ","date":"2022-03-30","objectID":"/text_classification/:3:2","tags":["project","NLP"],"title":"Multi_Class_Text_Classification","uri":"/text_classification/"},{"categories":["machine learning"],"content":"Model for Data Generation ","date":"2022-03-30","objectID":"/text_classification/:4:0","tags":["project","NLP"],"title":"Multi_Class_Text_Classification","uri":"/text_classification/"},{"categories":["machine learning"],"content":"Machine Learning ","date":"2022-03-30","objectID":"/text_classification/:5:0","tags":["project","NLP"],"title":"Multi_Class_Text_Classification","uri":"/text_classification/"},{"categories":["machine learning"],"content":"SVC ","date":"2022-03-30","objectID":"/text_classification/:5:1","tags":["project","NLP"],"title":"Multi_Class_Text_Classification","uri":"/text_classification/"},{"categories":["machine learning"],"content":"Logistics Regression ","date":"2022-03-30","objectID":"/text_classification/:5:2","tags":["project","NLP"],"title":"Multi_Class_Text_Classification","uri":"/text_classification/"},{"categories":["machine learning"],"content":"KNN ","date":"2022-03-30","objectID":"/text_classification/:5:3","tags":["project","NLP"],"title":"Multi_Class_Text_Classification","uri":"/text_classification/"},{"categories":["machine learning"],"content":"MultinomialNB ","date":"2022-03-30","objectID":"/text_classification/:5:4","tags":["project","NLP"],"title":"Multi_Class_Text_Classification","uri":"/text_classification/"},{"categories":["machine learning"],"content":"Data Balancing ","date":"2022-03-30","objectID":"/text_classification/:6:0","tags":["project","NLP"],"title":"Multi_Class_Text_Classification","uri":"/text_classification/"},{"categories":["machine learning"],"content":"Over Sampling ","date":"2022-03-30","objectID":"/text_classification/:6:1","tags":["project","NLP"],"title":"Multi_Class_Text_Classification","uri":"/text_classification/"},{"categories":["machine learning"],"content":"Under Sampling Machine Learning SVC Logistics Regression KNN MultinomialNB Ensemble other model ","date":"2022-03-30","objectID":"/text_classification/:6:2","tags":["project","NLP"],"title":"Multi_Class_Text_Classification","uri":"/text_classification/"},{"categories":["machine learning"],"content":"NLTK ","date":"2022-03-30","objectID":"/text_classification/:7:0","tags":["project","NLP"],"title":"Multi_Class_Text_Classification","uri":"/text_classification/"},{"categories":["machine learning"],"content":"NN ","date":"2022-03-30","objectID":"/text_classification/:8:0","tags":["project","NLP"],"title":"Multi_Class_Text_Classification","uri":"/text_classification/"},{"categories":["machine learning"],"content":"BERT ","date":"2022-03-30","objectID":"/text_classification/:9:0","tags":["project","NLP"],"title":"Multi_Class_Text_Classification","uri":"/text_classification/"},{"categories":["machine learning"],"content":"Multi Layer Neural Network ","date":"2022-03-30","objectID":"/text_classification/:10:0","tags":["project","NLP"],"title":"Multi_Class_Text_Classification","uri":"/text_classification/"},{"categories":["machine learning"],"content":"CNN ","date":"2022-03-30","objectID":"/text_classification/:11:0","tags":["project","NLP"],"title":"Multi_Class_Text_Classification","uri":"/text_classification/"},{"categories":["machine learning"],"content":"LSTM ","date":"2022-03-30","objectID":"/text_classification/:12:0","tags":["project","NLP"],"title":"Multi_Class_Text_Classification","uri":"/text_classification/"},{"categories":["machine learning"],"content":"Confusion Matrix ","date":"2022-03-30","objectID":"/text_classification/:13:0","tags":["project","NLP"],"title":"Multi_Class_Text_Classification","uri":"/text_classification/"},{"categories":["machine learning"],"content":"SUBMISSION ","date":"2022-03-30","objectID":"/text_classification/:14:0","tags":["project","NLP"],"title":"Multi_Class_Text_Classification","uri":"/text_classification/"},{"categories":["machine learning"],"content":"Conclusion ","date":"2022-03-30","objectID":"/text_classification/:15:0","tags":["project","NLP"],"title":"Multi_Class_Text_Classification","uri":"/text_classification/"},{"categories":null,"content":" A clap S hihat D kick F openhat G boom H ride J snare K tom L tink ","date":"2021-10-14","objectID":"/drum/:0:0","tags":null,"title":"Drum","uri":"/drum/"},{"categories":null,"content":" One of the Biggest Festival, Hindu celebrate. ","date":"2021-10-07","objectID":"/fes1/:0:0","tags":null,"title":"Dashain : Nine form of Goddess Durga","uri":"/fes1/"},{"categories":null,"content":"Day 1 Maa Shailaputri Name Shailaputri Image Meaning Daughter of Mountain Father and Mother Himavan and Menavati Husband Lord Shiva Recarnarion Sati(Parvati) Chakra Root Chakra(Muladhara Charkra) Mount(Vahan) Nandi bull Colour Yellow Mantra ॐ देवी शैलपुत्र्यै नमः॥ Om Devi Shailaputryai Namah । Prayer वन्दे वाञ्छितलाभाय चन्द्रार्ध कृतशेखराम् ॥ वृषारूढाम् शूलधराम् शैलपुत्रीम् यशस्विनीम् । या देवी सर्वभू‍तेषु माँ शैलपुत्री रूपेण संस्थिता । नमस्तस्यै नमस्तस्यै नमस्तस्यै नमो नमः ॥ Description Portrayed with a half-moon on her forehead. A Trident in her right hand along with a lotus in her left hand. She is seen as riding on the back of a bull Worship On Partipada Tithi, with Chandan, Flowers, Incense, Lamp and Fruits ","date":"2021-10-07","objectID":"/fes1/:1:0","tags":null,"title":"Dashain : Nine form of Goddess Durga","uri":"/fes1/"},{"categories":null,"content":"Day 2 Maa Brahmacharini Name Brahmacharini Image Meaning A Female Student (Who Pursues The Sacred Religious Knowledge) Father and Mother Himavan and Menavati Husband *** Recarnarion Goddess Parvati Unmarried Form Chakra Sacral Chakra(Svadhishthana Charkra) Mount(Vahan) *** Colour White(Sign of Purity) Flower Jasmine \u0026 lotuses Mantra ॐ देवी ब्रह्मचारिण्यै नम: Oṃ Devī Brahmacāriṇyai Namaḥ: Prayer दधाना करपद्माभ्यामक्षमालाकमण्डलू। देवी प्रसीदतु मयि ब्रह्मचारिण्यनुत्तमा॥ या देवी सर्वभू‍तेषु माँ ब्रह्मचारिणी रूपेण संस्थिता। नमस्तस्यै नमस्तस्यै नमस्तस्यै नमो नमः॥ Description Depicted as Walking On Bare Foot wears White Clothes and Crown. Jap Mala(Rudraksh Rosary) on her right hand along with Kamandal on her left hand. Worship On second day of Navratri. Maa Brahmacharini is worshipped along with Lord Shiva, by Offering Milk, Flowers, rice and sandalwood. ","date":"2021-10-07","objectID":"/fes1/:2:0","tags":null,"title":"Dashain : Nine form of Goddess Durga","uri":"/fes1/"},{"categories":null,"content":"Day 3 Maa Chandraghanta Name Chandraghanta Devi Image Meaning Always ready for war against demons and have half Chandra on her forehead. Father and Mother Himavan and Menavati Husband Lord Shiv Recarnarion Goddess Parvati Chakra Manipura Chakra Mount(Vahan) Tiger(Leopard) Colour Red Flower Jasmine Mantra ॐ देवी चन्द्रघण्टायै नमः॥ Om Devi Chandraghantayai Namah॥ Prayer पिण्डज प्रवरारूढा चण्डकोपास्त्रकैर्युता। प्रसादं तनुते मह्यम् चन्द्रघण्टेति विश्रुता॥ या देवी सर्वभू‍तेषु माँ चन्द्रघण्टा रूपेण संस्थिता। नमस्तस्यै नमस्तस्यै नमस्तस्यै नमो नमः॥ Description She has ten hands where two hands hold a Trishula(trident), Gada(mace), bow-arrow, khadak(sword), Kamala(lotus flower), Ghanta(bell) and kamandal(waterpot), while one of her hands remains in blessing posture.she wears a half moon depicting a Bell on her forehead and has a third eye in the middle of her forehead. Worship On Ashwin Sukla Paksha Tritiya. Maa Chandraghanta Devi, by Offering Water, Milk, Honey, Flowers and sandalwood. ","date":"2021-10-07","objectID":"/fes1/:3:0","tags":null,"title":"Dashain : Nine form of Goddess Durga","uri":"/fes1/"},{"categories":null,"content":"Day 4 Kushmanda Devi Name Kushmanda Devi Image Meaning Ku means “a little”, Ushma means “warmth” or “energy” and Anda means “cosmic egg”. Father and Mother *** Husband Lord Shiv Recarnarion Goddess Parvati Chakra Anahata Chakra Mount(Vahan) Lion Colour Red Flower Red color flowers Mantra ॐ देवी कूष्माण्डायै नमः॥ Om Devi Kushmandayai Namah॥ Prayer सुरासम्पूर्ण कलशं रुधिराप्लुतमेव च।। दधाना हस्तपद्माभ्यां कूष्माण्डा शुभदास्तु मे॥ या देवी सर्वभू‍तेषु माँ कूष्माण्डा रूपेण संस्थिता। नमस्तस्यै नमस्तस्यै नमस्तस्यै नमो नमः॥ Description Kushmanda is depicted with eight to ten hands holding a trident, discus, sword, hook, mace, bow, arrow and two jars of honey (Elixir) and blood. Her one hand is always on abhayamudra from which she blesses all her devotees. Worship On this day, as far as possible, a bright married woman with a big forehead should be worshipped. It is preferable to feed them curd, pudding in their food. After this, fruits, dry fruits and good luck items should be offered. Due to which the mother is pleased and the desired results are obtained. ","date":"2021-10-07","objectID":"/fes1/:4:0","tags":null,"title":"Dashain : Nine form of Goddess Durga","uri":"/fes1/"},{"categories":null,"content":"Day 5 Skandamata Devi Name Skandamata Devi Image Meaning Mata Parvati was given this name after giving birth to Lord Skanda, also known as Kartikeya. Father and Mother Himavan and Menavati Husband Lord Shiv Recarnarion Goddess Parvati Chakra 5th chakra of Kundalini – Visuddha Chakra Mount(Vahan) Lion Colour Red Flower Red color flowers Mantra ॐ देवी स्कन्दमातायै नम: Oṃ Devī Skandamātāyai Namaḥ Prayer सिंहासनगता नित्यं पद्माश्रितकरद्वया। शुभदास्तु सदा देवी स्कन्दमाता यशस्विनी॥ या देवी सर्वभू‍तेषु माँ स्कन्दमाता रूपेण संस्थिता। नमस्तस्यै नमस्तस्यै नमस्तस्यै नमो नमः॥ Description She is pictured carrying Lotus flowers in her upper two hands, baby Murugan in one of her right hands, and Abhaya Mudra in the other.She is also seen holding Kartikey, a small child, on her lap. Worship By offering red flowers, Shringar Samagri ","date":"2021-10-07","objectID":"/fes1/:5:0","tags":null,"title":"Dashain : Nine form of Goddess Durga","uri":"/fes1/"},{"categories":null,"content":"Day 6 Katyayani Devi Name Katyayani Devi Image Meaning Ku means “a little”, Ushma means “warmth” or “energy” and Anda means “cosmic egg”. Father and Mother *** Husband Lord Shiv Recarnarion Goddess Parvati Chakra Ajna Chakra Mount(Vahan) Lion Colour Red Flower Red color flowers Mantra ॐ देवी कूष्माण्डायै नमः॥ Om Devi Kushmandayai Namah॥ Prayer सुरासम्पूर्ण कलशं रुधिराप्लुतमेव च।। दधाना हस्तपद्माभ्यां कूष्माण्डा शुभदास्तु मे॥ या देवी सर्वभू‍तेषु माँ कूष्माण्डा रूपेण संस्थिता। नमस्तस्यै नमस्तस्यै नमस्तस्यै नमो नमः॥ Description Kushmanda is depicted with eight to ten hands holding a trident, discus, sword, hook, mace, bow, arrow and two jars of honey (Elixir) and blood. Her one hand is always on abhayamudra from which she blesses all her devotees. Worship On this day, as far as possible, a bright married woman with a big forehead should be worshipped. It is preferable to feed them curd, pudding in their food. After this, fruits, dry fruits and good luck items should be offered. Due to which the mother is pleased and the desired results are obtained. ","date":"2021-10-07","objectID":"/fes1/:6:0","tags":null,"title":"Dashain : Nine form of Goddess Durga","uri":"/fes1/"},{"categories":null,"content":"Day 7 Kalaratri Devi Name Kalaratri Devi Image Meaning Kaalratri; Kala(First part) means black in honour of being the first creation before the light itself. ratri( second part) means darkness after the sun sets, which whole combined to give the meaning the Goddess who is empowers the devotee with the devouring quality of kala (time) and the all-consuming nature of ratri (night) - allowing all obstacles to be overcome and guaranteeing success in all undertakings. Father and Mother *** Husband Lord Shiv Recarnarion Goddess Durga Chakra Crown Chakra Mount(Vahan) Donkey Colour Red Flower Red color flowers Mantra ॐ देवी कालरात्र्यै नम:॥ Oṃ Devī Kālarātryai Namaḥ॥ Prayer एकवेणी जपाकर्णपूरा नग्ना खरास्थिता लंंबोष्टी कर्णिकाकर्णी तैलाभ्यक्तशरीरिणी। वामपादोल्लसल्लोहलताकंठकभूषणा वर्धनमूर्धध्वजा कृष्णा कालरात्रिर्भयंकरी।। Description Kalaratri devi is depicted with four hands holding a trident, hook with two hand positioning blessing( giving and protecting) devotees. Worship On this day, as far as possible, a bright married woman with a big forehead should be worshipped. It is preferable to feed them curd, pudding in their food. After this, fruits, dry fruits and good luck items should be offered. Due to which the mother is pleased and the desired results are obtained. ","date":"2021-10-07","objectID":"/fes1/:7:0","tags":null,"title":"Dashain : Nine form of Goddess Durga","uri":"/fes1/"},{"categories":null,"content":"Day 8 Mahagauri Devi Name Mahagauri Devi Image Meaning Goddess of Beauty and Women. The name Mahagauri means extremely bright, clean complexioned, with a shine like the moon. (Mahā, महा = great; Gaurī, गौरी = bright, clean). Father and Mother *** Husband Lord Shiv Siblings Ganga and Vishnu Recarnarion Goddess Durga Chakra *** Chakra Mount(Vahan) Ox(White Bull) Colour Red Flower Red color flowers Mantra **ॐ देवी महागौर्यै नमः॥ ** Om Devi Mahagauryai Namah॥ Prayer Sarvamangal Mangalye, Shiva Sarvartha Sadhike Sharanye Tryambake Gauri Narayani Namostute Shwete Vrushe Samarudha Shvetambardhara Shuchih Mahagauri shubham dadyanmahadev-pramod-da Description According to a legend, to get Lord Shiva in her husband’s form, the goddess did severe penance, due to which her body turns black. Pleased with the penance of the goddess, Lord Shiva accepts her and Shiva washes her body with Ganga-water. Then the goddess becomes very radiant like white light, and since then she was named ‘Gauri’. Goddess Mahagauri is also known as Annapurna, Aishwarya Pradayini, Chaitanyamai.Mahagauri has four arms. His vehicle is Taurus. He holds Abhaya Mudra in his upper right hand and Trishul in his lower right hand. There is a Damru in the upper left hand and Vara-mudra in the lower left hand. His posture is very calm. Worship Mahagauri is worshiped on the eighth day of Navratri. According to Hinduism, Goddess Mahagauri has the power to fulfill all the wishes of her devotees. One who worships the goddess gets freedom from all the troubles of life. On the day of Ashtami, women offer chunari to the mother goddess for their honeymoon. The law of worship of Goddess Gauri is also the same, that is, just as you have worshiped the mother till the date of Saptami, similarly on the day of Ashtami, like every day, worship the Goddess with Panchopchar. ","date":"2021-10-07","objectID":"/fes1/:8:0","tags":null,"title":"Dashain : Nine form of Goddess Durga","uri":"/fes1/"},{"categories":null,"content":"Day 9 Siddhidatri Devi Name Siddhidatri Devi Image Meaning Siddhi means supernatural power or meditative ability, and Dhatri means giver or awarder. Father and Mother *** Husband Lord Shiv Recarnarion Goddess Durga Chakra Goddess who governs the planet Ketu. Mount(Vahan) Lotus Colour Pink Flower Red color flowers Mantra ‘ॐ देवी सिद्धिदात्र्यै नमः॥’ **Om Hreem Shri Siddhidatri Durgaaye Namaha ** Prayer सिद्धगन्धर्वयक्षाघैरसुरैरमरैरपि। सेव्यमाना सदा भूयात् सिद्धिदा सिद्धिदायिनी॥ वन्दे वांछित मनोरथार्थ चन्द्रार्घकृत शेखराम्। कमलस्थितां चतुर्भुजा सिद्धीदात्री यशस्वनीम्॥ स्वर्णावर्णा निर्वाणचक्रस्थितां नवम् दुर्गा त्रिनेत्राम्। शख, चक्र, गदा, पदम, धरां सिद्धीदात्री भजेम्॥ नमस्तस्यै नमस्तस्यै नमस्तस्यै नमो नमः॥ Description Siddhidhatri is the moola roopa of goddess Parvati. She has four hands holding a discus, conch shell, mace, and lotus. She is sitting on a fully bloomed lotus or a lion. She possesses eight supernatural powers, or the siddhis, called Anima, Mahima, Garima, Laghima, Prapti, Prakambya, Ishitva and Vashitva. Anima means reducing one’s body to the size of an atom; Mahima means expanding one’s body to an infinitely large size; Garima means becoming infinitely heavy; Laghima means becoming weightless; Prapti means having omnipresence; Prakambya achieving whatever one desire; Ishitva means possessing absolute lordship; and Vashitva means having the power to subjugate all. Lord Shiva was blessed by Siddhidatri by being given by all the eight powers. Worship On this day, Butter and Khoi (sweet made with popped rice) are unique offerings made to Devi Siddhidatri on the final day of Navratri Pooja to receive her unconditional love and fulfillment of your desires. ","date":"2021-10-07","objectID":"/fes1/:9:0","tags":null,"title":"Dashain : Nine form of Goddess Durga","uri":"/fes1/"},{"categories":["NEPAL STOCK"],"content":"This article shows the nepal company status.","date":"2021-09-01","objectID":"/test/","tags":["NEPSE","SHARE MARKET"],"title":"Nepal Company Status","uri":"/test/"},{"categories":["NEPAL STOCK"],"content":"In order to view the advance real time nepsa data as well as other nepse related things please visit –\u003e nepse. This website run an automatic command on github to take data from nepse related website and display data. nepse.sumityadav.com.np ","date":"2021-09-01","objectID":"/test/:0:0","tags":["NEPSE","SHARE MARKET"],"title":"Nepal Company Status","uri":"/test/"},{"categories":["NEPAL STOCK"],"content":"Today GRAPH This below is the graph of taday nepse value with other day value. ","date":"2021-09-01","objectID":"/test/:1:0","tags":["NEPSE","SHARE MARKET"],"title":"Nepal Company Status","uri":"/test/"},{"categories":null,"content":" Course Structure for Bachelor’s Degree in Computer Engineering Here I presented the TU, IOE Syllabus of BCT. ","date":"2021-08-19","objectID":"/syllabus/:0:0","tags":null,"title":"Syllabus of BCT","uri":"/syllabus/"},{"categories":null,"content":"1. Year: 1, Semester: Odd Code Title Exam Type Theory Ass Theory Final Practical Ass Practical Final Total SH401 Engineering Mathematics I T 20 80 100 CT401 Computer Programming B 20 80 50 150 ME401 Engineering Drawing I P 60 40 100 SH402 Engineering Physics B 20 80 20 30 150 CE401 Applied Mechanics T 20 80 100 EE401 Basic Electrical Engineering B 20 80 25 125 Total Marks 725 ","date":"2021-08-19","objectID":"/syllabus/:1:0","tags":null,"title":"Syllabus of BCT","uri":"/syllabus/"},{"categories":null,"content":"2. Year: 1, Semester: Even Code Title Exam Type Theory Ass Theory Final Practical Ass Practical Final Total SH451 Engineering Mathematics II T 20 80 100 ME451 Engineering Drawing II P 60 40 100 EX451 Basic Electronics Engineering B 20 80 25 125 SH453 Engineering Chemistry B 20 80 20 30 150 ME452 Fundamental of Thermodynamics \u0026 Heat Transfer B 20 80 25 125 ME453 Workshop Technology B 10 40 50 Total Marks 650 ","date":"2021-08-19","objectID":"/syllabus/:2:0","tags":null,"title":"Syllabus of BCT","uri":"/syllabus/"},{"categories":null,"content":"3. Year: 2, Semester: Odd Code Title Exam Type Theory Ass Theory Final Practical Ass Practical Final Total SH501 Engineering Mathematics III T 20 80 100 CT501 Object Oriented Programming B 20 80 50 150 EE501 Electric Circuit Theory B 20 80 25 125 CT502 Theory of Computation T 20 80 100 EX501 Electronic Devices \u0026 Circuits B 20 80 25 125 EX502 Digital Logic B 20 80 50 150 EX503 Electromagnetics B 20 80 25 125 Total Marks 875 ","date":"2021-08-19","objectID":"/syllabus/:3:0","tags":null,"title":"Syllabus of BCT","uri":"/syllabus/"},{"categories":null,"content":"4. Year: 2, Semester: Even Code Title Exam Type Theory Ass Theory Final Practical Ass Practical Final Total EE554 Electrical Machine B 20 80 25 125 SH553 Numerical Methods B 20 80 50 150 SH551 Applied Mathematics T 20 80 100 EE552 Instrumentation I B 20 80 25 125 CT552 Data Structure \u0026 Algorithm B 20 80 50 150 EX551 Microprocessor B 20 80 50 150 CT551 Discrete Structure T 20 80 100 Total Marks 900 ","date":"2021-08-19","objectID":"/syllabus/:4:0","tags":null,"title":"Syllabus of BCT","uri":"/syllabus/"},{"categories":null,"content":"5. Year: 3, Semester: Odd Code Title Exam Type Theory Ass Theory Final Practical Ass Practical Final Total SH601 Communication English B 20 80 25 125 CT601 Software Engineering B 20 80 25 125 SH602 Probability \u0026 Statistics T 20 80 100 EX602 Instrumentation II B 20 80 25 125 EX603 Computer Graphics B 20 80 50 150 CT602 Data Communication B 20 80 25 125 CT603 Computer Orginization \u0026 Architecture B 20 80 25 125 Total Marks 875 ","date":"2021-08-19","objectID":"/syllabus/:5:0","tags":null,"title":"Syllabus of BCT","uri":"/syllabus/"},{"categories":null,"content":"6. Year: 3, Semester: Even Code Title Exam Type Theory Ass Theory Final Practical Ass Practical Final Total CE655 Engineering Economics T 20 80 100 CT655 Embedded System B 20 80 25 125 CT653 Artificial Intelligence B 20 80 25 125 CT652 Database Management System B 20 80 50 150 CT651 Object Oriented Analysis \u0026 Design B 20 80 25 125 CT656 Operating System B 20 80 25 125 CT654 Minor Project P 50 25 75 Total Marks 825 ","date":"2021-08-19","objectID":"/syllabus/:6:0","tags":null,"title":"Syllabus of BCT","uri":"/syllabus/"},{"categories":null,"content":"7. Year: 4, Semester: Odd Code Title Exam Type Theory Ass Theory Final Practical Ass Practical Final Total CT701 Project Management T 20 80 100 CT702 Computer Network B 20 80 50 150 CT703 Distributed System B 20 80 25 125 CT704 Digital Signal Analysis \u0026 Processing B 20 80 25 125 CT707 Project I P 50 50 EX701 Energy Environment \u0026 Society T 10 40 50 ME708 Organization \u0026 Management T 20 80 100 CT72501 Advanced Java Programming (Elective I) B 20 80 25 125 CT72502 Data Mining (Elective I) B 20 80 25 125 CT72503 Embeded System Design Using ARM Technology (Elective I) B 20 80 25 125 CT72504 Image Processing \u0026 Pattern Recognition (Elective I) B 20 80 25 125 CT72505 Web Technology (Elective I) B 20 80 25 125 EE72504 Power Electronics (Elective I) B 20 80 25 125 EX72501 Radar Technology (Elective I) B 20 80 25 125 EX72502 Satellite Communication (Elective I) B 20 80 25 125 EX72503 Biomedical Instrumentation (Elective I) B 20 80 25 125 EX72504 Aeronautical Telecommunication (Elective I) B 20 80 25 125 EX72505 RF \u0026 Microwave Engineering (Elective I) B 20 80 25 125 Total Marks 825 ","date":"2021-08-19","objectID":"/syllabus/:7:0","tags":null,"title":"Syllabus of BCT","uri":"/syllabus/"},{"categories":null,"content":"8. Year: 4, Semester: Even Code Title Exam Type Theory Ass Theory Final Practical Ass Practical Final Total CE752 Engineering Professional Practice T 10 40 50 CT751 Information Systems T 20 80 100 CT753 Simulation \u0026 Modeling B 20 80 25 125 CT754 Internet \u0026 Intranet B 20 80 25 125 CT755 Project II P 50 50 100 CT76502 Agile Methodologies (Elective II) B 20 80 25 125 CT76503 Networking with IPv6 (Elective II) B 20 80 25 125 CT76507 Big Data Technologies (Elective II) B 20 80 25 125 CT78501 Remote Sensing (Elective III) B 20 80 25 125 CT78503 Multimedia System (Elective III) B 20 80 25 125 CT78504 Enterprise Application Design \u0026 Development (Elective III) B 20 80 25 125 CT78505 XML: Foundation Techniques \u0026 Applications (Elective III) B 20 80 25 125 EX78503 Telecommunication (Elective III) B 20 80 25 125 CT78507 Geographic Information System (Elective III) B 20 80 25 125 CT78508 Speech Processing (Elective III) B 20 80 25 125 Total Marks 750 ","date":"2021-08-19","objectID":"/syllabus/:8:0","tags":null,"title":"Syllabus of BCT","uri":"/syllabus/"},{"categories":null,"content":"PDF FILE BCT PDF ","date":"2021-08-19","objectID":"/syllabus/:9:0","tags":null,"title":"Syllabus of BCT","uri":"/syllabus/"},{"categories":["machine learning"],"content":"Nepali OCR detector In this post I am going to present Nepali Optical Character Recognition (OCR) that extracts Nepali text from images and scanned documents so that it can be edited, formatted, indexed, searched, or translated. Below mentioned code are written in python, using easyocr as a heart of this post. ","date":"2021-08-16","objectID":"/nepali_ocr/:0:0","tags":["project","OCR"],"title":"Nepali_OCR_Python","uri":"/nepali_ocr/"},{"categories":["machine learning"],"content":"OCR Optical character recognition or optical character reader (OCR) is the electronic or mechanical conversion of images of typed, handwritten or printed text into machine-encoded text, whether from a scanned document, a photo of a document, a scene-photo (for example the text on signs and billboards in a landscape photo) or from subtitle text superimposed on an image (for example: from a television broadcast). Widely used as a form of data entry from printed paper data records – whether passport documents, invoices, bank statements, computerized receipts, business cards, mail, printouts of static-data, or any suitable documentation – it is a common method of digitizing printed texts so that they can be electronically edited, searched, stored more compactly, displayed on-line, and used in machine processes such as cognitive computing, machine translation, (extracted) text-to-speech, key data and text mining. OCR is a field of research in pattern recognition, artificial intelligence and computer vision. Early versions needed to be trained with images of each character, and worked on one font at a time. Advanced systems capable of producing a high degree of recognition accuracy for most fonts are now common, and with support for a variety of digital image file format inputs. Some systems are capable of reproducing formatted output that closely approximates the original page including images, columns, and other non-textual components.More about OCR ","date":"2021-08-16","objectID":"/nepali_ocr/:0:1","tags":["project","OCR"],"title":"Nepali_OCR_Python","uri":"/nepali_ocr/"},{"categories":["machine learning"],"content":"importing Module #!pip install easyocr #To install easyocr import matplotlib.pyplot as plt import cv2 import easyocr import numpy as np from pylab import rcParams from IPython.display import Image rcParams['figure.figsize'] = 8, 16 ","date":"2021-08-16","objectID":"/nepali_ocr/:1:0","tags":["project","OCR"],"title":"Nepali_OCR_Python","uri":"/nepali_ocr/"},{"categories":["machine learning"],"content":"Loading pre trained model. ’ne’ for Nepali and ’en’ for english and simillary for other reader = easyocr.Reader(['ne']) #'ne' for Nepali and 'en' for english and simillary for other ","date":"2021-08-16","objectID":"/nepali_ocr/:2:0","tags":["project","OCR"],"title":"Nepali_OCR_Python","uri":"/nepali_ocr/"},{"categories":["machine learning"],"content":"Normal image to Scaned image def map(x, in_min, in_max, out_min, out_max): return (x - in_min) * (out_max - out_min) / (in_max - in_min) + out_min def highPassFilter(img,kSize): if not kSize%2: kSize +=1 kernel = np.ones((kSize,kSize),np.float32)/(kSize*kSize) filtered = cv2.filter2D(img,-1,kernel) filtered = img.astype('float32') - filtered.astype('float32') filtered = filtered + 127*np.ones(img.shape, np.uint8) filtered = filtered.astype('uint8') return filtered def blackPointSelect(img, blackPoint): img = img.astype('int32') img = map(img, blackPoint, 255, 0, 255) _, img = cv2.threshold(img, 0, 255, cv2.THRESH_TOZERO) img = img.astype('uint8') return img def whitePointSelect(img,whitePoint): _,img = cv2.threshold(img, whitePoint, 255, cv2.THRESH_TRUNC) img = img.astype('int32') img = map(img, 0, whitePoint, 0, 255) img = img.astype('uint8') return img def blackAndWhite(img): lab = cv2.cvtColor(img, cv2.COLOR_BGR2LAB) (l,a,b) = cv2.split(lab) img = cv2.add( cv2.subtract(l,b), cv2.subtract(l,a) ) return img def scan_effect(img): blackPoint = 66 whitePoint = 130 image = highPassFilter(img,kSize = 51) image_white = whitePointSelect(image, whitePoint) img_black = blackPointSelect(image_white, blackPoint) image=blackPointSelect(img,blackPoint) white = whitePointSelect(image,whitePoint) img_black = blackAndWhite(white) return img_black Enter the location of image file loc=\"2.jpg\" #Enter the loction of image Image(loc) img = cv2.imread(loc) image = scan_effect(img) #from google.colab.patches import cv2_imshow #cv2_imshow(image) filename = 'scanned.jpg' cv2.imwrite(filename, image) True ","date":"2021-08-16","objectID":"/nepali_ocr/:3:0","tags":["project","OCR"],"title":"Nepali_OCR_Python","uri":"/nepali_ocr/"},{"categories":["machine learning"],"content":"Loading Image path=filename Image(path) ","date":"2021-08-16","objectID":"/nepali_ocr/:4:0","tags":["project","OCR"],"title":"Nepali_OCR_Python","uri":"/nepali_ocr/"},{"categories":["machine learning"],"content":"Detecting character from image output = reader.readtext(path) ","date":"2021-08-16","objectID":"/nepali_ocr/:5:0","tags":["project","OCR"],"title":"Nepali_OCR_Python","uri":"/nepali_ocr/"},{"categories":["machine learning"],"content":"Output output [([[123, 1], [667, 1], [667, 67], [123, 67]], 'त्या बांच्न के ढुकुर झैं नभमा डुलेर', 0.3381714640956552), ([[129, 59], [409, 59], [409, 128], [129, 128]], 'त्यो मर्नु के कुकुर', 0.6069389645579002), ([[424, 60], [663, 60], [663, 116], [424, 116]], 'झैं पथमा ढलेर', 0.678598319341647), ([[141, 121], [655, 121], [655, 185], [141, 185]], 'त्याे जित्न के मनपरि दुनिया ठगेर', 0.466542709532707), ([[134, 180], [656, 180], [656, 236], [134, 236]], 'बांचौं पवित्र जलको सरिता बनरे ।', 0.33612146115405894), ([[151, 267], [647, 267], [647, 329], [151, 329]], 'गर्दै कि बुद्घ जसरी सबको भलाइ', 0.5689838746811848), ([[138, 329], [294, 329], [294, 380], [138, 380]], 'वा ज्ञानले', 0.5712557530269956), ([[428, 325], [639, 325], [639, 392], [428, 392]], 'दुनियां जिगाई', 0.5282788595967648), ([[135, 383], [667, 383], [667, 445], [135, 445]], 'वा कृष्ण झैं जगत्मा रमिता चलाई', 0.6665105167823989), ([[136, 462], [176, 462], [176, 494], [136, 494]], 'वा', 0.41052963629001243), ([[181, 441], [673, 441], [673, 501], [181, 501]], 'विश्वमाझ सबमा ममता फिजाई', 0.6406025192944045), ([[151, 535], [275, 535], [275, 579], [151, 579]], 'सग्लो र', 0.7202154126778443), ([[278, 519], [659, 519], [659, 594], [278, 594]], 'पूर्ण सपना मनभित्र सांचौं', 0.8457427886612249), ([[169, 577], [644, 577], [644, 645], [169, 645]], 'छोटो छ है समयमै भरपुर बांचौं', 0.4758441227091572), ([[151, 637], [659, 637], [659, 701], [151, 701]], 'हांस्ने गरोस् मन सध्रै दुनियां भुलेर', 0.5020757275372445), ([[134, 698], [676, 698], [676, 754], [134, 754]], 'डाहा गरोस् जनता नै रिसमा जलेर ।', 0.4316845329398307), ([[161, 777], [653, 777], [653, 839], [161, 839]], 'सत्कर्मले जगत्को शिरमा पुगिन्छ', 0.7593451815519658), ([[308, 834], [659, 834], [659, 888], [308, 888]], 'हदयका सब दाग बिल्छ', 0.8728011010651449), ([[159, 889], [661, 889], [661, 949], [159, 949]], 'आर्दश जीवन सधैं अति पूज्य हुन्छ', 0.5903392799676969), ([[135, 946], [683, 946], [683, 1006], [135, 1006]], 'सत्कर्म सन्ततिहरू सबमा फिजिन्छ ।।।', 0.493018168522814), ([[134, 1030], [418, 1030], [418, 1082], [134, 1082]], 'जो रम्छ व्यर्थपनमा', 0.8048376333582101), ([[443, 1024], [702, 1024], [702, 1084], [443, 1084]], 'त्यसमै सकिन्छन्', 0.5815147920443708), ([[134, 1086], [700, 1086], [700, 1143], [134, 1143]], 'जो बुझछ जीवन उनी इतिहास बन्छन्', 0.7268483165161402), ([[163, 1138], [664, 1138], [664, 1192], [163, 1192]], 'झक्नेछ ईश्वर पनि उसको अगाडि', 0.47465339469231005), ([[124, 1192], [708, 1192], [708, 1246], [124, 1246]], 'जो बांच्छ स्वच्छ दिलले सबलाई बांडी ।०', 0.41452520111176533), ([[295.2103500411843, 334.2731201317898], [429.4015153514063, 324.3877917562871], [430.7896499588157, 377.7268798682102], [296.5984846485937, 388.6122082437129]], 'जनकभैं', 0.8082762642642637), ([[160.20510460345565, 850.3332719689847], [304.5957251500903, 837.0304035071042], [306.79489539654435, 885.6667280310153], [162.4042748499097, 897.9695964928958]], 'मुस्कानले', 0.9778876236551189)] ","date":"2021-08-16","objectID":"/nepali_ocr/:6:0","tags":["project","OCR"],"title":"Nepali_OCR_Python","uri":"/nepali_ocr/"},{"categories":["machine learning"],"content":"Total detection print(f'Total number of detection',len(output)) Total number of detection 27 ","date":"2021-08-16","objectID":"/nepali_ocr/:7:0","tags":["project","OCR"],"title":"Nepali_OCR_Python","uri":"/nepali_ocr/"},{"categories":["machine learning"],"content":"Previewing Output image = cv2.imread(path) for i in range(len(output)): cord = output[i][0] x_min, y_min = [int(min(idx)) for idx in zip(*cord)] x_max, y_max = [int(max(idx)) for idx in zip(*cord)] cv2.rectangle(image,(x_min,y_min),(x_max,y_max),(0,0,255),2) print(output[i][1]) plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB)) त्या बांच्न के ढुकुर झैं नभमा डुलेर त्यो मर्नु के कुकुर झैं पथमा ढलेर त्याे जित्न के मनपरि दुनिया ठगेर बांचौं पवित्र जलको सरिता बनरे । गर्दै कि बुद्घ जसरी सबको भलाइ वा ज्ञानले दुनियां जिगाई वा कृष्ण झैं जगत्मा रमिता चलाई वा विश्वमाझ सबमा ममता फिजाई सग्लो र पूर्ण सपना मनभित्र सांचौं छोटो छ है समयमै भरपुर बांचौं हांस्ने गरोस् मन सध्रै दुनियां भुलेर डाहा गरोस् जनता नै रिसमा जलेर । सत्कर्मले जगत्को शिरमा पुगिन्छ हदयका सब दाग बिल्छ आर्दश जीवन सधैं अति पूज्य हुन्छ सत्कर्म सन्ततिहरू सबमा फिजिन्छ ।।। जो रम्छ व्यर्थपनमा त्यसमै सकिन्छन् जो बुझछ जीवन उनी इतिहास बन्छन् झक्नेछ ईश्वर पनि उसको अगाडि जो बांच्छ स्वच्छ दिलले सबलाई बांडी ।० जनकभैं मुस्कानले \u003cmatplotlib.image.AxesImage at 0x7ff354990ad0\u003e ","date":"2021-08-16","objectID":"/nepali_ocr/:8:0","tags":["project","OCR"],"title":"Nepali_OCR_Python","uri":"/nepali_ocr/"},{"categories":["machine learning"],"content":"Summary In this way we sucessfully Completed this shot project which was of detecting Nepali words. Find the GOOGLE COLAB HERE Free free to Star this code on Github. ","date":"2021-08-16","objectID":"/nepali_ocr/:9:0","tags":["project","OCR"],"title":"Nepali_OCR_Python","uri":"/nepali_ocr/"},{"categories":["Math"],"content":"Discover what the Hugo - LoveIt theme is all about and the core-concepts behind it.","date":"2021-07-22","objectID":"/julia_set_fractal/","tags":["Basic","wallpaper"],"title":"Julia_Set_Fractal","uri":"/julia_set_fractal/"},{"categories":["Math"],"content":" Julia Set The Julia set is named after the French mathematician Gaston Julia who investigated their properties circa 1915 and culminated in his famous paper in 1918. While the Julia set is now associated with a simpler polynomial, Julia was interested in the iterative properties of a more general expression, namely z4 + z3/(z-1) + z2/(z3 + 4 z2 + 5) + c. The Julia set is now associated with those points z = x + iy on the complex plane for which the series zn+1 = zn2 + c does not tend to infinity. c is a complex constant, one gets a different Julia set for each c. The initial value z0 for the series is each point in the image plane. In the broader sense the exact form of the iterated function may be anything, the general form being zn+1 = f(zn), interesting sets arise with non-linear functions f(z). Commonly used functions include the following: $$z(n+1) = c sin(zn) zn+1 = c exp(zn) $$zn+1 = c i cos(zn) zn+1 = c zn (1 - zn) Computing a Julia set by computer is straightforward, at least by the brute force method presented here. The image is created by mapping each pixel to a rectangular region of the complex plane. Each pixel then represents the starting point for the series, z0. The series is computed for each pixel and if it diverges to infinity it is drawn in white, if it doesn’t then it is drawn black. This convergence or otherwise isn’t always obvious and it may take a large number of iterations to resolve so a decision procedure is required to determine divergence. This typically involves assuming the series tends to infinity as soon as its value exceeds some value, if the series hasn’t diverged after a certain number of terms it is similarly assigned to be part of the set. Both these decisions can be varied to give more precise images but ones that take longer to calculate. An added effect is achieved by colouring the point by how fast it diverges to infinity. The well known Mandelbrot set forms a kind of index into the Julia set. A Julia set is either connected or disconnected, values of c chosen from within the Mandelbrot set are connected while those from the outside of the Mandelbrot set are disconnected. The disconnected sets are often called “dust”, they consist of individual points no matter what resolution they are viewed at.Above Mentioned Line are taken from This website. ","date":"2021-07-22","objectID":"/julia_set_fractal/:0:0","tags":["Basic","wallpaper"],"title":"Julia_Set_Fractal","uri":"/julia_set_fractal/"},{"categories":["Math"],"content":"CODE import numpy as np import matplotlib.pyplot as plt import matplotlib.cm as cm # Image width and height; parameters for the plot im_width, im_height = 5000, 5000 rel = float(input(\"enter the real part\")) img = float(input(\"enter the img part\")) c = complex(rel, img) zabs_max = 10 nit_max = 1000 xmin, xmax = -1.5, 1.5 xwidth = xmax - xmin ymin, ymax = -1.5, 1.5 yheight = ymax - ymin julia = np.zeros((im_width, im_height)) for ix in range(im_width): for iy in range(im_height): nit = 0 # Map pixel position to a point in the complex plane z = complex(ix / im_width * xwidth + xmin, iy / im_height * yheight + ymin) # Do the iterations while abs(z) \u003c= zabs_max and nit \u003c nit_max: z = z**2 + c nit += 1 shade = 1-np.sqrt(nit / nit_max) ratio = nit / nit_max julia[ix,iy] = ratio enter the real part-0.54 enter the img part0.54 ","date":"2021-07-22","objectID":"/julia_set_fractal/:1:0","tags":["Basic","wallpaper"],"title":"Julia_Set_Fractal","uri":"/julia_set_fractal/"},{"categories":["Math"],"content":"IMAGES ","date":"2021-07-22","objectID":"/julia_set_fractal/:2:0","tags":["Basic","wallpaper"],"title":"Julia_Set_Fractal","uri":"/julia_set_fractal/"},{"categories":["Math"],"content":"IMAGE 1 fig, ax = plt.subplots(figsize=(50,50)) ax.imshow(julia, interpolation='nearest', cmap=cm.hot) plt.show() ","date":"2021-07-22","objectID":"/julia_set_fractal/:2:1","tags":["Basic","wallpaper"],"title":"Julia_Set_Fractal","uri":"/julia_set_fractal/"},{"categories":["Math"],"content":"IMAGE 2 fig, ax = plt.subplots(figsize=(50,50)) ax.imshow(julia, interpolation='nearest', cmap=cm.hot) plt.show() ","date":"2021-07-22","objectID":"/julia_set_fractal/:2:2","tags":["Basic","wallpaper"],"title":"Julia_Set_Fractal","uri":"/julia_set_fractal/"},{"categories":["Math"],"content":"IMAGE 3 fig, ax = plt.subplots(figsize=(50,50)) ax.imshow(julia, interpolation='nearest', cmap=cm.hot) plt.show() ","date":"2021-07-22","objectID":"/julia_set_fractal/:2:3","tags":["Basic","wallpaper"],"title":"Julia_Set_Fractal","uri":"/julia_set_fractal/"},{"categories":["Math"],"content":"IMAGE 4 fig, ax = plt.subplots(figsize=(50,50)) ax.imshow(julia, interpolation='nearest', cmap=cm.hot) plt.show() ","date":"2021-07-22","objectID":"/julia_set_fractal/:2:4","tags":["Basic","wallpaper"],"title":"Julia_Set_Fractal","uri":"/julia_set_fractal/"},{"categories":["Math"],"content":"IMAGE 5 fig, ax = plt.subplots(figsize=(50,50)) ax.imshow(julia, interpolation='nearest', cmap=cm.hot) plt.show() ","date":"2021-07-22","objectID":"/julia_set_fractal/:2:5","tags":["Basic","wallpaper"],"title":"Julia_Set_Fractal","uri":"/julia_set_fractal/"},{"categories":["Machine Learning"],"content":" AUTO ENCODER In this notebook you will find how auto encoder are trained in tensorflow(Keras). This notebook have two different type of encoder i.e normal * Auto Encoder * and * Denoising Auto Encoder * . ","date":"2021-07-19","objectID":"/autoencoder/:0:0","tags":["ML","project","Supervised"],"title":"AutoEncoder","uri":"/autoencoder/"},{"categories":["Machine Learning"],"content":"Normal Encoder LIBRARY ARE LOADED %matplotlib inline %config InlineBackend.figure_format = 'retina' import matplotlib.pyplot as plt import pandas as pd import numpy as np import seaborn as sns import warnings warnings.filterwarnings('ignore') #from __future__ import print_function from keras.models import Model from keras.layers import Dense, Input from keras.datasets import mnist from keras.regularizers import l1 from keras.optimizers import Adam FOR PLOTING OUTPUTS def plot_autoencoder_outputs(autoencoder, n, dims): decoded_imgs = autoencoder.predict(x_test) # number of example digits to show n = 5 plt.figure(figsize=(10, 4.5)) for i in range(n): # plot original image ax = plt.subplot(2, n, i + 1) plt.imshow(x_test[i].reshape(*dims)) plt.gray() ax.get_xaxis().set_visible(False) ax.get_yaxis().set_visible(False) if i == n/2: ax.set_title('Original Images') # plot reconstruction ax = plt.subplot(2, n, i + 1 + n) plt.imshow(decoded_imgs[i].reshape(*dims)) plt.gray() ax.get_xaxis().set_visible(False) ax.get_yaxis().set_visible(False) if i == n/2: ax.set_title('Reconstructed Images') plt.show() TRAIN TEST SPLITS (x_train, y_train), (x_test, y_test) = mnist.load_data() x_train = x_train.astype('float32') / 255.0 x_test = x_test.astype('float32') / 255.0 RESIZED FOR PROCESSING x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:]))) x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:]))) print(x_train.shape) print(x_test.shape) (60000, 784) (10000, 784) MODEL CREATION input_size = 784 hidden_size = 128 code_size = 32 input_img = Input(shape=(input_size,)) hidden_1 = Dense(hidden_size, activation='relu')(input_img) code = Dense(code_size, activation='relu')(hidden_1) hidden_2 = Dense(hidden_size, activation='relu')(code) output_img = Dense(input_size, activation='sigmoid')(hidden_2) autoencoder = Model(input_img, output_img) autoencoder.compile(optimizer='adam', loss='binary_crossentropy') autoencoder.fit(x_train, x_train, epochs=3) Epoch 1/3 1875/1875 [==============================] - 29s 2ms/step - loss: 0.1880 Epoch 2/3 1875/1875 [==============================] - 5s 3ms/step - loss: 0.1003 Epoch 3/3 1875/1875 [==============================] - 4s 2ms/step - loss: 0.0950 \u003ckeras.callbacks.History at 0x7fc58b7709d0\u003e MODEL SUMMARY(ARC.) autoencoder.summary() Model: \"model\" _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) [(None, 784)] 0 _________________________________________________________________ dense (Dense) (None, 128) 100480 _________________________________________________________________ dense_1 (Dense) (None, 32) 4128 _________________________________________________________________ dense_2 (Dense) (None, 128) 4224 _________________________________________________________________ dense_3 (Dense) (None, 784) 101136 ================================================================= Total params: 209,968 Trainable params: 209,968 Non-trainable params: 0 _________________________________________________________________ MODEL OUTPUT plot_autoencoder_outputs(autoencoder, 5, (28, 28)) MODEL TEST l=np.zeros((28,12)) m=np.ones((28,4))*0.8 r=np.zeros((28,12)) I=np.concatenate((l,m,r),axis=1).reshape(1,784) plt.imshow(I.reshape(28,28)) L= autoencoder.predict(I) print(L.shape) plt.figure(figsize=(8, 8)) plt.imshow(L.reshape(28,28)) (1, 784) \u003cmatplotlib.image.AxesImage at 0x7fc57026c700\u003e MODEL WEIGHTS VISUALIZATION weights = autoencoder.get_weights()[0].T n = 10 plt.figure(figsize=(20, 5)) for i in range(n): ax = plt.subplot(1, n, i + 1) plt.imshow(weights[i+0].reshape(28, 28)) ax.get_xaxis().set_visible(False) ax.get_yaxis().set_visible(False) ","date":"2021-07-19","objectID":"/autoencoder/:1:0","tags":["ML","project","Supervised"],"title":"AutoEncoder","uri":"/autoencoder/"},{"categories":["Machine Learning"],"content":"DENOISING AUTO ENCODER DATA CREATION noise_factor = 0.4 x_train_noisy = x_train + noise_factor * np.random.normal(size=x_train.shape) x_test_noisy = x_test + noise_factor * np.random.normal(size=x_test.shape) x_train_noisy = np.clip(x_train_noisy, 0.0, 1.0) x_test_noisy = np.clip(x_test_noisy, 0.0, 1.0) n = 5 plt.figure(figsize=(10, 4.5)) for i in range(n): # plot original image ax = plt.subplot(2, n, i + 1) plt.imshow(x_test[i].reshape(28, 28)) plt.gray() ax.get_xaxis().set_visible(False) ax.get_yaxis().set_visible(False) if i == n/2: ax.set_title('Original Images') # plot noisy image ax = plt.subplot(2, n, i + 1 + n) plt.imshow(x_test_noisy[i].reshape(28, 28)) plt.gray() ax.get_xaxis().set_visible(False) ax.get_yaxis().set_visible(False) if i == n/2: ax.set_title('Noisy Input') MODEL input_size = 784 hidden_size = 128 code_size = 32 input_img = Input(shape=(input_size,)) hidden_1 = Dense(hidden_size, activation='relu')(input_img) code = Dense(code_size, activation='relu')(hidden_1) hidden_2 = Dense(hidden_size, activation='relu')(code) output_img = Dense(input_size, activation='sigmoid')(hidden_2) autoencoder = Model(input_img, output_img) autoencoder.compile(optimizer='adam', loss='binary_crossentropy') autoencoder.fit(x_train_noisy, x_train, epochs=10) Epoch 1/10 1875/1875 [==============================] - 7s 2ms/step - loss: 0.2093 Epoch 2/10 1875/1875 [==============================] - 3s 2ms/step - loss: 0.1309 Epoch 3/10 1875/1875 [==============================] - 4s 2ms/step - loss: 0.1219 Epoch 4/10 1875/1875 [==============================] - 3s 2ms/step - loss: 0.1181 Epoch 5/10 1875/1875 [==============================] - 3s 2ms/step - loss: 0.1161 Epoch 6/10 1875/1875 [==============================] - 5s 2ms/step - loss: 0.1143 Epoch 7/10 1875/1875 [==============================] - 5s 2ms/step - loss: 0.1132 Epoch 8/10 1875/1875 [==============================] - 4s 2ms/step - loss: 0.1124 Epoch 9/10 1875/1875 [==============================] - 3s 2ms/step - loss: 0.1115 Epoch 10/10 1875/1875 [==============================] - 4s 2ms/step - loss: 0.1113 \u003ckeras.callbacks.History at 0x7fc570466790\u003e PLOTING DATA n = 5 plt.figure(figsize=(10, 7)) images = autoencoder.predict(x_test_noisy) for i in range(n): # plot original image ax = plt.subplot(3, n, i + 1) plt.imshow(x_test[i].reshape(28, 28)) plt.gray() ax.get_xaxis().set_visible(False) ax.get_yaxis().set_visible(False) if i == n/2: ax.set_title('Original Images') # plot noisy image ax = plt.subplot(3, n, i + 1 + n) plt.imshow(x_test_noisy[i].reshape(28, 28)) plt.gray() ax.get_xaxis().set_visible(False) ax.get_yaxis().set_visible(False) if i == n/2: ax.set_title('Noisy Input') # plot noisy image ax = plt.subplot(3, n, i + 1 + 2*n) plt.imshow(images[i].reshape(28, 28)) plt.gray() ax.get_xaxis().set_visible(False) ax.get_yaxis().set_visible(False) if i == n/2: ax.set_title('Autoencoder Output') WEIGHT VISUALIZATION weights = autoencoder.get_weights()[0].T n = 10 plt.figure(figsize=(20, 5)) for i in range(n): ax = plt.subplot(1, n, i + 1) plt.imshow(weights[i+0].reshape(28, 28)) ax.get_xaxis().set_visible(False) ax.get_yaxis().set_visible(False) THANKS ","date":"2021-07-19","objectID":"/autoencoder/:2:0","tags":["ML","project","Supervised"],"title":"AutoEncoder","uri":"/autoencoder/"},{"categories":["math"],"content":"Math and Python","date":"2021-07-10","objectID":"/simplemath/","tags":["math","Python"],"title":"Simple Math","uri":"/simplemath/"},{"categories":["math"],"content":" NUMPY AND PYTHON DERIVATIVE OF polynomial function import numpy as np def basis_function(x): a=np.eye(x) b=np.zeros((x,1)) c=np. concatenate((b,a),axis=1) d=np.array(range(1,x+1)).reshape(x,1) e=c *d return e enter coeff.of equation in given pattern :- constant,x,x^2,x^3,…. #ENTER HERE [\\\\\\\\\\\\\\\\\\\\\\\\\\] d=np.array([34,90,34,90]) #NUMBER MUST BE SEPERATED BY COMMA d print(f'your equation is {d[0]}+{d[1]}x +{d[2]}x^2-----') #TO FIND LENGTH e=len(d) y=e-1 #RESHAPE d=d.reshape(e,1) #CALLING BASIS FUNCTION f=basis_function(y) #FINDING DERIVATIVE g=f@d #PRINTING OUT print(f'your derivative coeff. in order of constant,x,x^2,x^3,.... term is:- {g.reshape(1,-1)} ') Result your equation is 90+90x +90x^2—– your derivative coeff. in order of constant,x,x^2,x^3,…. term is:- [[ 90. 180. 270.]] ","date":"2021-07-10","objectID":"/simplemath/:0:0","tags":["math","Python"],"title":"Simple Math","uri":"/simplemath/"},{"categories":["machine Learning"],"content":"Generative Model","date":"2021-07-09","objectID":"/supervised/","tags":["ML","Convolution","Generative","GAN"],"title":"Unsupervised","uri":"/supervised/"},{"categories":["machine Learning"],"content":" VAE VAE TYPES ","date":"2021-07-09","objectID":"/supervised/:0:0","tags":["ML","Convolution","Generative","GAN"],"title":"Unsupervised","uri":"/supervised/"},{"categories":["machine Learning"],"content":"Auto Encoder This is like a simple generative algorithm in which one model is used to reduce the data like simple compression and other is for increasing data like decompressor. This model contain different neural network model for both of task. Here there is difficult to develop or generate determined image from compressed laten space. ","date":"2021-07-09","objectID":"/supervised/:1:0","tags":["ML","Convolution","Generative","GAN"],"title":"Unsupervised","uri":"/supervised/"},{"categories":["machine Learning"],"content":"Variational Autoencoders Up to now, we have discussed dimensionality reduction problem and introduce autoencoders that are encoder-decoder architectures that can be trained by gradient descent. Let’s now make the link with the content generation problem, see the limitations of autoencoders in their current form for this problem and introduce Variational Autoencoders. Limitations of autoencoders for content generation * At this point, a natural question that comes in mind is “what is the link between autoencoders and content generation?”. Indeed, once the autoencoder has been trained, we have both an encoder and a decoder but still no real way to produce any new content. At first sight, we could be tempted to think that, if the latent space is regular enough (well “organized” by the encoder during the training process), we could take a point randomly from that latent space and decode it to get a new content. The decoder would then act more or less like the generator of a Generative Adversarial Network. How data is generated in VAE. autoencoder vs VAE LOSS FOR VAE WITH REGULARISATION GAN GAN TYPES ","date":"2021-07-09","objectID":"/supervised/:2:0","tags":["ML","Convolution","Generative","GAN"],"title":"Unsupervised","uri":"/supervised/"},{"categories":["machine Learning"],"content":"Machine Learning Model for Classification","date":"2021-06-21","objectID":"/first_post/","tags":["ML","Convolution","project"],"title":"Nepali_Devanagari_Letter_Classifier","uri":"/first_post/"},{"categories":["machine Learning"],"content":" In the search of deep data analytic we comes to deep machine learnig which is most famous way of finding pattern in data and predicting the future state of data pattern. In this python based machine learning code i have written a convolution based deep learnig code of predicting and recogination of number,letter and and many more. In this code i have used tranfer learning way of predicting number and letter in real time by using c++ based one of the famous visual modulus callled Open CV. Our model are quit good in recorgining letter and number. I have trained my program in my local data set i.e devanagari data set. You can find all code in my github ","date":"2021-06-21","objectID":"/first_post/:0:0","tags":["ML","Convolution","project"],"title":"Nepali_Devanagari_Letter_Classifier","uri":"/first_post/"},{"categories":["Thought"],"content":"Story of Boy","date":"2021-06-21","objectID":"/second_post/","tags":["lesson","quotes"],"title":"Sentence That Define Me","uri":"/second_post/"},{"categories":["Thought"],"content":"Hi! In this post i will put some quotes and some historic line which defined me and suggest me what to do in that very critical stage. ","date":"2021-06-21","objectID":"/second_post/:0:0","tags":["lesson","quotes"],"title":"Sentence That Define Me","uri":"/second_post/"},{"categories":["Thought"],"content":"From my Twitter ","date":"2021-06-21","objectID":"/second_post/:1:0","tags":["lesson","quotes"],"title":"Sentence That Define Me","uri":"/second_post/"},{"categories":["Thought"],"content":"Its Magic! ( Wave to image and image to wave) If it can be sampled then can be compressible. Image compressing using Fourier Transform. #fft2 #Matlab pic.twitter.com/eV4Y0TTKjc — Sumit Yadav (@Rocker_Ritesh) June 11, 2022 ","date":"2021-06-21","objectID":"/second_post/:1:1","tags":["lesson","quotes"],"title":"Sentence That Define Me","uri":"/second_post/"},{"categories":["Thought"],"content":"Very silent style is yours, should I annihilate or love your style. #foryou #quotes pic.twitter.com/jAaoSnpREN — Sumit Yadav (@Rocker_Ritesh) August 15, 2021 ","date":"2021-06-21","objectID":"/second_post/:1:2","tags":["lesson","quotes"],"title":"Sentence That Define Me","uri":"/second_post/"},{"categories":["Thought"],"content":"I have found eternal Aloneness. — Sumit Yadav (@Rocker_Ritesh) August 13, 2021 ","date":"2021-06-21","objectID":"/second_post/:1:3","tags":["lesson","quotes"],"title":"Sentence That Define Me","uri":"/second_post/"},{"categories":["Thought"],"content":"One obvious thing is.Being tensioned after thinking about future. pic.twitter.com/xvVY7xvMEO — Sumit Yadav (@Rocker_Ritesh) July 1, 2021 ","date":"2021-06-21","objectID":"/second_post/:1:4","tags":["lesson","quotes"],"title":"Sentence That Define Me","uri":"/second_post/"},{"categories":["Thought"],"content":"You are a different being!!! pic.twitter.com/ABkhMr3oU5 — Sumit Yadav (@Rocker_Ritesh) July 5, 2021 ","date":"2021-06-21","objectID":"/second_post/:1:5","tags":["lesson","quotes"],"title":"Sentence That Define Me","uri":"/second_post/"},{"categories":["Thought"],"content":"You live in me like a recurring dream. pic.twitter.com/Gj6L5FVR3A — Sumit Yadav (@Rocker_Ritesh) July 27, 2021 ","date":"2021-06-21","objectID":"/second_post/:1:6","tags":["lesson","quotes"],"title":"Sentence That Define Me","uri":"/second_post/"},{"categories":["Thought"],"content":"Reflection is illusion when there is no lights. Objects become illusion when there is no fate. #janakimandir pic.twitter.com/dMjBP0qpek — Sumit Yadav (@Rocker_Ritesh) July 30, 2021 ","date":"2021-06-21","objectID":"/second_post/:1:7","tags":["lesson","quotes"],"title":"Sentence That Define Me","uri":"/second_post/"},{"categories":["Thought"],"content":"your eyes doesn't seems like a merchant as you are. — Sumit Yadav (@Rocker_Ritesh) August 1, 2021 ","date":"2021-06-21","objectID":"/second_post/:1:8","tags":["lesson","quotes"],"title":"Sentence That Define Me","uri":"/second_post/"},{"categories":["Thought"],"content":"Lets survive, and find meaning of suffering which we get when we are alive. — Sumit Yadav (@Rocker_Ritesh) August 7, 2021 ","date":"2021-06-21","objectID":"/second_post/:1:9","tags":["lesson","quotes"],"title":"Sentence That Define Me","uri":"/second_post/"},{"categories":["Thought"],"content":"Our Journey of love had concluded this way We were heart-broken but undefeated. — Sumit Yadav (@Rocker_Ritesh) August 11, 2021 ","date":"2021-06-21","objectID":"/second_post/:1:10","tags":["lesson","quotes"],"title":"Sentence That Define Me","uri":"/second_post/"},{"categories":["Thought"],"content":"Tomorrow seems more shorter than today. — Sumit Yadav (@Rocker_Ritesh) June 25, 2021 ","date":"2021-06-21","objectID":"/second_post/:1:11","tags":["lesson","quotes"],"title":"Sentence That Define Me","uri":"/second_post/"},{"categories":["Thought"],"content":"This unexpected wind started extinguishing my heart in my dream too. #ThoughtForTheDay pic.twitter.com/L7lnZgkCi4 — Sumit Yadav (@Rocker_Ritesh) June 7, 2021 ","date":"2021-06-21","objectID":"/second_post/:1:12","tags":["lesson","quotes"],"title":"Sentence That Define Me","uri":"/second_post/"},{"categories":["Thought"],"content":"\"No waking age\" — Sumit Yadav (@Rocker_Ritesh) May 29, 2021 ","date":"2021-06-21","objectID":"/second_post/:1:13","tags":["lesson","quotes"],"title":"Sentence That Define Me","uri":"/second_post/"},{"categories":["Thought"],"content":"Centuries will remember your infidelity! — Sumit Yadav (@Rocker_Ritesh) May 25, 2021 ","date":"2021-06-21","objectID":"/second_post/:1:14","tags":["lesson","quotes"],"title":"Sentence That Define Me","uri":"/second_post/"},{"categories":["Thought"],"content":"Be a general in Love, not an Ordinary. #love — Sumit Yadav (@Rocker_Ritesh) May 20, 2021 ","date":"2021-06-21","objectID":"/second_post/:1:15","tags":["lesson","quotes"],"title":"Sentence That Define Me","uri":"/second_post/"},{"categories":["Thought"],"content":"You are the best version of yourself who can perfectly describe you! pic.twitter.com/zfzTS13Mfk — Sumit Yadav (@Rocker_Ritesh) May 14, 2021 ","date":"2021-06-21","objectID":"/second_post/:1:16","tags":["lesson","quotes"],"title":"Sentence That Define Me","uri":"/second_post/"},{"categories":["Thought"],"content":"Sun set is not end, It's start of new Hope.#janakimandir #eveningview pic.twitter.com/z64qAOfAps — Sumit Yadav (@Rocker_Ritesh) April 24, 2021 ","date":"2021-06-21","objectID":"/second_post/:1:17","tags":["lesson","quotes"],"title":"Sentence That Define Me","uri":"/second_post/"},{"categories":["Thought"],"content":"In the search of success I forget to sacrify. pic.twitter.com/9rQ58sLwX5 — Sumit Yadav (@Rocker_Ritesh) April 19, 2021 ","date":"2021-06-21","objectID":"/second_post/:1:18","tags":["lesson","quotes"],"title":"Sentence That Define Me","uri":"/second_post/"},{"categories":["Thought"],"content":"\"No waking age\" — Sumit Yadav (@Rocker_Ritesh) May 29, 2021 ","date":"2021-06-21","objectID":"/second_post/:1:19","tags":["lesson","quotes"],"title":"Sentence That Define Me","uri":"/second_post/"},{"categories":["Thought"],"content":"I am Black not Dark, U may be White but not Bright.#racism #motivate pic.twitter.com/Y7oWnOX8n6 — Sumit Yadav (@Rocker_Ritesh) April 15, 2021 ","date":"2021-06-21","objectID":"/second_post/:1:20","tags":["lesson","quotes"],"title":"Sentence That Define Me","uri":"/second_post/"},{"categories":["Thought"],"content":"From Holy Book ","date":"2021-06-21","objectID":"/second_post/:2:0","tags":["lesson","quotes"],"title":"Sentence That Define Me","uri":"/second_post/"},{"categories":["Thought"],"content":"\u003e ","date":"2021-06-21","objectID":"/second_post/:2:1","tags":["lesson","quotes"],"title":"Sentence That Define Me","uri":"/second_post/"}]