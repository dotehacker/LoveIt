# Tensor_processing_units

<body class="c52"><div><p class="c24 c10"><span class="c0"></span></p><p class="c10 c28 c43"><span class="c0"></span></p></div><p class="c17 c10"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 882.67px;"><img alt="" src="/images/image9.png" style="width: 624.00px; height: 882.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c33"><a class="c9" href="https://www.google.com/url?q=https://sumityadav.com.np/&amp;sa=D&amp;source=editors&amp;ust=1654780423459483&amp;usg=AOvVaw1tnkp3Sqm7pG6C-2MBmNzn">Sumit Yadav</a></span><span class="c0">&nbsp;(PUL076BCT088)</span></p><p class="c17 c10"><span class="c33"><a class="c9" href="https://www.google.com/url?q=http://doece.pcampus.edu.np/index.php/prof-dr-subarna-shakya/&amp;sa=D&amp;source=editors&amp;ust=1654780423460000&amp;usg=AOvVaw0n00kKkurQCC6rLUL2lgvw">Prof. Dr. &nbsp;Subarna Shakya</a></span><span class="c33 c45"><a class="c9" href="https://www.google.com/url?q=http://doece.pcampus.edu.np/index.php/prof-dr-subarna-shakya/&amp;sa=D&amp;source=editors&amp;ust=1654780423460209&amp;usg=AOvVaw3S30Pih8eKGkajBDOLxp_e">&nbsp;</a></span></p><p class="c17 c10"><span>Director, Information Technology Innovation Center, Tribhuvan University</span></p><p class="c17 c10"><span>Computer Organization and Architecture</span></p><p class="c17 c10"><span>09</span><span class="c45">&nbsp;</span><span>June</span><span class="c45">&nbsp;20</span><span class="c0">22</span></p><p class="c10 c51 title" id="h.bllyran0q013"><span class="c15 c50">TPUs (Tensor Processing Unit)</span></p><p class="c1"><span>Tensor Processing Units (TPUs) are custom-designed application-specific integrated circuits (ASICs) developed by Google to accelerate AI &nbsp;and neural network works. Formally, it was begun in 2015 and first made available publicly &nbsp;in 2018 for both cloud computing and small business. Tensor processing unit was first introduced in 2016 at Google I/O, when the company said the TPU had been already used in the company for over a year. This chip was quite specific to machine learning framework like TensorFlow, PyTorch and JAX. TPU is another kind of processing unit like a CPU or a GPU. There are, however, some big differences between those - the biggest difference being that TPU is an ASIC. And you probably know CPU and GPU are not, as they are not optimized to do one specific kind of application.</span></p><h1 class="c25 c10" id="h.t03ncj2rfsbl"><span class="c15">CPU, GPU and TPU</span></h1><p class="c17 c10"><span class="c0">Now let&#39;s compare the difference between CPU, GPU and TPU. (For the case of multiple addition)</span></p><ol class="c38 lst-kix_7khs5xrsqq99-0 start" start="1"><li class="c10 c13 li-bullet-0"><span class="c0">A CPU performs the multiply-add operation by reading each input and weight from memory, multiplying them with its ALU (Arithmetic Logic Unit), writing them back to memory and finally adding up all the multiplies values. Modern CPUs are strengthened by massive cache, branch prediction and high clock rate on each of its cores. Which all contribute to a lower latency of the CPU.</span></li><li class="c13 c10 li-bullet-0"><span class="c0">A GPU does however not use the fancy features which lower the latency. It also needs to orchestrate its thousands of ALUs which further decreases the latency. In short, GPUs drastically increase its throughput by parallelizing its computation in exchange for an increase in its latency.</span></li><li class="c13 c10 li-bullet-0"><span class="c0">A TPU on the other hand operates very differently. Its ALUs are directly connected to each other without using the memory. They can directly give pass information, which will drastically decrease latency.</span></li></ol><p class="c17 c10"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 141.33px;"><img alt="" src="/images/image16.png" style="width: 624.00px; height: 141.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c17 c10"><span class="c27">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c47">Table 1. Benchmarked servers use Haswell CPUs, K80 GPUs, and TPUs. Haswell has 18 cores, and the K80 has 13 SMX processors.</span></p><p class="c25 c10"><span class="c0">Now on the basics of workload (While training AI models):-</span></p><p class="c25 c10 c28"><span class="c0"></span></p><ul class="c38 lst-kix_jhu0nrslc0b9-0 start"><li class="c25 c10 c42 c37 li-bullet-0"><span class="c0">CPUs</span></li></ul><ul class="c38 lst-kix_jhu0nrslc0b9-1 start"><li class="c12 c10 li-bullet-0"><span class="c0">Quick prototyping that requires maximum flexibility</span></li><li class="c12 c10 li-bullet-0"><span class="c0">Simple models that do not take long to train</span></li><li class="c12 c10 li-bullet-0"><span class="c0">Models that are dominated by custom TensorFlow operation written in C++</span></li><li class="c12 c10 li-bullet-0"><span class="c0">Models that are &nbsp;limited by available I/O or the networking bandwidth of the host machine.</span></li></ul><ul class="c38 lst-kix_jhu0nrslc0b9-0"><li class="c25 c10 c37 c42 li-bullet-0"><span class="c0">GPUs</span></li></ul><ul class="c38 lst-kix_jhu0nrslc0b9-1 start"><li class="c10 c12 li-bullet-0"><span class="c0">Models for which source does not exist or is too onerous to change</span></li><li class="c12 c10 li-bullet-0"><span class="c0">Models with a significant number of custom TensorFlow operations that must run at least partially on CPUs</span></li><li class="c12 c10 li-bullet-0"><span class="c0">Models with TensorFlow ops that are not available on Cloud TPU</span></li><li class="c12 c10 li-bullet-0"><span class="c0">Medium-to-large models with larger effective batch sizes</span></li></ul><ul class="c38 lst-kix_jhu0nrslc0b9-0"><li class="c25 c10 c42 c37 li-bullet-0"><span class="c0">TPUs</span></li></ul><ul class="c38 lst-kix_jhu0nrslc0b9-1 start"><li class="c12 c10 li-bullet-0"><span class="c0">Models dominated by matrix computation</span></li><li class="c12 c10 li-bullet-0"><span class="c0">Models with no custom TensorFlow operations inside the main training loop</span></li><li class="c12 c10 li-bullet-0"><span class="c0">Models that train for weeks or months</span></li><li class="c12 c10 li-bullet-0"><span>Larger and very large models with very larger effective batch sizes</span></li></ul><h1 class="c25 c10" id="
h.spzkk67m1110"><span class="c40 c15">TPU Origin, Architecture, and Implementation</span></h1><p class="c17 c10"><span class="c0">Starting as early as 2006, we discussed deploying GPUs, FPGAs, or custom ASICs in our datacenters. We concluded that the few applications that could run on special hardware could be done virtually for free using the excess capacity of our large datacenters, and it&rsquo;s hard to improve on free. The conversation changed in 2013 when a projection where people use voice search for 3 minutes a day using speech recognition DNNs would require our datacenters to double to meet computation demands, which would be very expensive to satisfy with conventional CPUs. Thus, we started a high-priority project to quickly produce a custom ASIC for inference (and bought off-the-shelf GPUs for training). The goal was to improve cost-performance by 10X over GPUs. Given this mandate, the TPU was designed, verified, built, and deployed in datacenters in just 15 months.</span></p><p class="c17 c10"><span class="c0">Rather than be tightly integrated with a CPU, to reduce the chances of delaying deployment, the TPU was designed to be a coprocessor on the PCIe I/O bus, allowing it to plug into existing servers just as a GPU does. Moreover, to simplify hardware design and debugging, the host server sends TPU instructions for it to execute rather than fetching them itself. Hence, the TPU is closer in spirit to an FPU (floating-point unit) coprocessor than it is to a GPU.</span></p><p class="c6 c10"><span class="c0"></span></p><p class="c17 c10"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 637.50px;"><img alt="" src="/images/image5.png" style="width: 624.00px; height: 637.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c17 c10"><span class="c29">Fig 1. TPU Block Diagram. The main computation part is the yellow Matrix Multiply unit in the upper right-hand corner. Its inputs are the blue Weight FIFO and the blue Unified Buffer(UB) and its output is the blue Accumulators(Acc). The yellow Activation Unit performs the nonlinear functions on the Acc, Which go to the UB.</span></p><p class="c17 c10"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 481.33px;"><img alt="" src="/images/image7.png" style="width: 624.00px; height: 481.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c17 c10"><span class="c29">Fig 2. Floor Plan of TPU die. The shading follow Figure 1. The light(blue) data buffers are 37% of the die, the light (yellow) compute is 30%, the medium (green) I/O is 10%, and the dark (red) control is just 2%. Control is much larger (and much more difficult to design) in CPU or GPU.</span></p><p class="c6 c10"><span class="c29"></span></p><p class="c17 c10"><span class="c21">The goal was to run whole inference models in the TPU to reduce interactions with the host CPU and to be flexible enough to match the NN needs of 2015 and beyond, instead of just what was required for 2013 NNs. Figure 1 shows the block diagram of the TPU.</span></p><p class="c17 c10"><span class="c32">The TPU instructions are sent from the host over the PCIe Gen3 x16 bus into an instruction buffer. The internal blocks are typically connected together by 256-</span><span class="c31">byte</span><span class="c32">-wide paths. Starting in the upper-right corner, the </span><span class="c31">Matrix Multiply Unit </span><span class="c32">is the heart of the TPU. It contains 256x256 MACs that can perform 8-bit multiply-and-adds on signed or unsigned integers. The 16-bit products are collected in the 4 MiB of 32-bit </span><span class="c31">Accumulators </span><span class="c21">below the matrix unit. The 4MiB represents 4096, 256-element, 32-bit accumulators. The matrix unit produces one 256-element partial sum per clock cycle. We picked 4096 by first noting that the operations per byte need to reach peak performance (roofline knee in Section 4) is ~1350, so we rounded that up to 2048 and then duplicated it so that the compiler could use double buffering while running at peak performance.</span></p><p class="c6 c10"><span class="c21"></span></p><p class="c17 c10"><span class="c32">When using a mix of 8-bit weights and 16-bit activations (or vice versa), the Matrix Unit computes at half-speed, and it computes at a quarter-speed when both are 16 bits. It reads and writes 256 values per clock cycle and can perform either a matrix multiply or a convolution. The matrix unit holds one 64 KiB tile of weights plus one for double-buffering (to hide the 256 cycles it takes to shift a tile in). This unit is designed for dense matrices. Sparse architectural support was omitted for time-to-deploy reasons. Sparsity will have high priority in future designs. The weights for the matrix unit are staged through an on-chip </span><span class="c31">Weight FIFO </span><span class="c32">that reads from an off-chip 8 GiB DRAM called </span><span class="c31">Weight Memory </span><span class="c32">(for inference, weights are read-only; 8 GiB supports many simultaneously active models). The weight FIFO is four tiles deep. The intermediate results are held in the 24 MiB on-chip </span><span class="c31">Unified Buffer</span><span class="c21">, which can serve as inputs to the Matrix Unit. A programmable DMA controller transfers data to or from CPU Host memory and the Unified Buffer. Figure 2 shows the floor plan of the TPU die. The 24 MiB Unified Buffer is almost a third of the die and the Matrix Multiply Unit is a quarter, so the datapath is nearly two-thirds of the die. The 24 MiB size was picked in part to match the pitch of the Matrix Unit on the die and, given the short development schedule, in part to simplify the compiler. Control is just 2%. Figure 3 shows the TPU on its printed circuit card, which inserts into existing servers like an SATA disk.</span></p><p class="c17 c10"><span class="c21">As instructions are sent over the relatively slow PCIe bus, TPU instructions follow the CISC tradition, including a repeat field. The average clock cycles per instruction (CPI) of these CISC instructions is typically 10 to 20. It has about a dozen instructions overall, but these five are the key ones:</span></p><p class="c6 c10"><span class="c21"></span></p><p class="c17 c10"><span class="c32">1. </span><span class="c11">Read_Host_Memory </span><span class="c21">reads data from the CPU host memory into the Unified Buffer (UB).</span></p><p class="c6 c10"><span class="c21"></span></p><p class="c17 c10"><span class="c32">2. </span><span class="c11">Read_Weights </span><span class="c21">reads weights from Weight Memory into the Weight FIFO as input to the Matrix Unit.</span></p><p class="c6 c10"><span class="c21"></span></p><p class="c17 c10"><span class="c32">3. </span><span class="c11">MatrixMultiply/Convolve </span><span class="c21">causes the Matrix Unit to perform a matrix multiply or a convolution from the Unified Buffer into the Accumulators. A matrix operation takes a variable-sized B*256 input, multiplies it by a 256x256 constant weight input, and produces a B*256 output, taking B pipelined cycles to complete.</span></p><p class="c6 c10"><span class="c21"></span></p><p class="c17 c10"><span class="c32">4. </span><span class="c11">Activate </span><span class="c21">performs the nonlinear function of the artificial neuron, with options for ReLU, Sigmoid, and so on. Its inputs are the Accumulators, and its output is the Unified Buffer. It can also perform the pooling operations needed for convolutions using the dedicated hardware on the die, as it is connected to nonlinear function logic.</span></p><p class="c6 c10"><span class="c21"></span></p><p class="c17 c10"><span class="c32">5. </span><span class="c11">Write_Host_Memory </span><span class="c21">writes data from the Unified Buffer into the CPU host memory.</span></p><p class="c17 c10"><span class="c21">&nbsp;The other instructions are alternate host memory read/write, set configuration, two </span></p><p class="c17 c10"><span class="c21">versions of synchronization, interrupt host, debug-tag, nop, and halt. The CISC MatrixMultiply instruction is 12 bytes, of which 3 are Unified Buffer address; 2 are</span></p><p class="c17 c10"><span class="c21">accumulator address; 4 are length (sometimes 2 dimensions for convolutions); and the rest are opcode and flags.</span></p><p class="c6 c10"><span class="c21"></span></p><p class="c17 c10"><span class="c32">The philosophy of the TPU microarchitecture is to keep the matrix unit busy. It uses a 4-stage pipeline for these CISC instructions, where each instruction executes in a separate stage. The plan was to hide the execution of the other instructions by overlapping their execution with the </span><span class="c11">MatrixMultiply </span><span class="c32">instruction. Toward that end, the </span><span class="c11">Read_Weights </span><span class="c21">instruction follows the decoupled-access/execute philosophy [Smi82], in that it can complete after sending its address but before the weight is fetched from Weight Memory. The matrix unit will stall if the input activation or weight data is not ready.</span></p><p class="c6 c10"><span class="c21"></span></p><p class="c17 c10"><span class="c21">We don&rsquo;t have clean pipeline overlap diagrams, because our CISC instructions can occupy a station for thousands of clock cycles, unlike the traditional RISC pipeline with one clock cycle per 
stage. Interesting cases occur when the activations for one network layer must complete before the matrix multiplications of the next layer can begin; we see a &ldquo;delay slot,&rdquo; where the matrix unit waits for explicit synchronization before safely reading from the Unified Buffer.</span></p><p class="c6 c10"><span class="c21"></span></p><p class="c17 c10"><span class="c21">&nbsp;As reading a large SRAM uses much more power than arithmetic, the matrix unit uses systolic execution to save energy by reducing reads and writes of the Unified Buffer . It relies on data from different directions arriving at cells in an array at regular intervals where they are combined. Figure 4 shows that data flows in from the left, and the weights are loaded from the top. A given 256-element multiply-accumulate operation moves through the matrix as a diagonal wavefront. The weights are preloaded, and take effect with the advancing wave alongside the first data of a new block. Control and data are pipelined to give the illusion that the 256 inputs are read at once, and that they instantly update one location of each of 256 accumulators. From a correctness perspective, software is unaware of the systolic nature of the matrix unit, but for performance, it does worry about the latency of the unit.</span></p><p class="c6 c10"><span class="c21"></span></p><p class="c17 c10"><span class="c21">The TPU software stack had to be compatible with those developed for CPUs and GPUs so that applications could be ported quickly to the TPU. The portion of the application run on the TPU is typically written in TensorFlow and is compiled into an API that can run on GPUs or TPUs [Lar16]. Like GPUs, the TPU stack is split into a User Space Driver and a Kernel Driver. The Kernel Driver is lightweight and handles only memory management and interrupts. It is designed for long-term stability. The User Space driver changes frequently. It sets up and controls TPU execution, reformats data into TPU order,</span></p><p class="c17 c10"><span class="c21">translates API calls into TPU instructions, and turns them into an application binary. The User Space driver compiles a model the first time it is evaluated, caching the program image and writing the weight image into the TPU&rsquo;s weight memory; the second and following evaluations run at full speed. The TPU runs most models completely from inputs to outputs, maximizing the ratio of TPU compute time to I/O time. Computation is often done one layer at a time, with overlapped execution allowing the matrix multiply unit to hide most non-critical-path operations.</span></p><p class="c6 c10"><span class="c21"></span></p><p class="c17 c10"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 499.50px;"><img alt="" src="/images/image12.png" style="width: 624.00px; height: 499.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c17 c10"><span class="c29">Fig 3. TPU Printed Circuit Board. It can be inserted in the slot for an SATA disk in server, but the card uses PCIe Gen3 x16.</span></p><p class="c17 c10"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 540.50px; height: 395.85px;"><img alt="" src="/images/image10.png" style="width: 540.50px; height: 395.85px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c17 c10"><span class="c29">Fig 4. Systolic data flow of the Matrix Multiply Unit. Software has the illusion that each 256B input is read at once, and they instantly update one location of each of 256 accumulators RAMs.</span></p><h1 class="c25 c10" id="h.u1vwjjptitno"><span class="c40 c15">UNDERSTANDING MORE</span></h1><p1 class="c10 c25" id="h.opaq22z6yvu8"><span class="c0">To understand why TPUs were designed the way, let&#39;s look at the calculations involved in running a simple neural network.</span></h1><h1 class="c25 c10" id="h.tpa3ezsu4dj9"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 310.17px;"><img alt="" src="/images/image4.png" style="width: 624.00px; height: 310.17px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></h1><p class="c1"><span class="c47">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Fig5. Double spiral problem on TensorFlow Playground (</span><span class="c33 c47"><a class="c9" href="https://www.google.com/url?q=http://playground.tensorflow.org/%23activation%3Drelu%26batchSize%3D10%26dataset%3Dspiral%26regDataset%3Dreg-plane%26learningRate%3D0.03%26regularizationRate%3D0%26noise%3D0%26networkShape%3D8,8,5%26seed%3D0.53586%26showTestData%3Dfalse%26discretize%3Dfalse%26percTrainData%3D50%26x%3Dtrue%26y%3Dtrue%26xTimesY%3Dfalse%26xSquared%3Dfalse%26ySquared%3Dfalse%26cosX%3Dfalse%26sinX%3Dfalse%26cosY%3Dfalse%26sinY%3Dfalse%26collectStats%3Dfalse%26problem%3Dclassification%26initZero%3Dfalse%26showTestData_hide%3Dtrue%26activation_hide%3Dtrue%26problem_hide%3Dtrue%26noise_hide%3Dtrue%26discretize_hide%3Dtrue%26regularization_hide%3Dtrue%26dataset_hide%3Dtrue%26batchSize_hide%3Dtrue%26learningRate_hide%3Dtrue%26regularizationRate_hide%3Dtrue%26percTrainData_hide%3Dtrue%26numHiddenLayers_hide%3Dtrue&amp;sa=D&amp;source=editors&amp;ust=1654780423467372&amp;usg=AOvVaw0g9SLArcMYf0KuaniIyZKA">click here</a></span><span class="c47">&nbsp;to try it)</span></p><p class="c1 c28"><span class="c0"></span></p><p class="c1"><span>This example on the TensorFlow Playground trains a neural network to classify a data point as blue or orange based on a training dataset. The process of running a trained neural network to classify data with labels or estimate some missing or future values is called </span><span class="c15">inference</span><span class="c0">. For inference, each neuron in a neural network does the following calculations:</span></p><p class="c1 c28"><span class="c0"></span></p><ul class="c38 lst-kix_xvmc87q669ih-0 start"><li class="c2 c42 li-bullet-0"><span class="c15">Multiply </span><span class="c0">the input data(x) with weights (w) to represent the signal strength</span></li></ul><p class="c2 c28"><span class="c0"></span></p><ul class="c38 lst-kix_xvmc87q669ih-0"><li class="c2 c42 li-bullet-0"><span class="c15">Add </span><span class="c0">the results to aggregate the neuron&rsquo;s state into a single value</span></li></ul><p class="c2 c28"><span class="c0"></span></p><ul class="c38 lst-kix_xvmc87q669ih-0"><li class="c2 c42 li-bullet-0"><span>Apply an </span><span class="c15">activation</span><span class="c0">&nbsp;function (f) to modulate the artificial neuron&rsquo;s activity.</span></li></ul><p class="c2 c28"><span class="c0"></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 279.00px; height: 153.50px;"><img alt="" src="/images/image13.png" style="width: 279.00px; height: 153.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c34 c10"><span>For example, if you have three inputs and two neurons with a fully connected single-layer neural network, you have to execute six multiplications between the weights and inputs and add up the multiplications in two groups of three. This sequence of multiplications and additions can be written as a </span><span class="c15">matrix multiplication</span><span class="c0">. The outputs of this matrix multiplication are then processed further by an activation function. Even when working with much more complex neural network model architectures, multiplying matrices is often the most computationally intensive part of running a trained model.</span></p><p class="c34 c10"><span class="c0">How many multiplication operations would you need at production scale? In July 2016, Google r surveyed six representative neural network applications across Google&rsquo;s production services and summed up the total number of weights in each neural network architecture. You can see the results in the table below.</span></p><p class="c34 c10 c28"><span class="c0"></span></p><a id="t.49dfdf6ef3114b9586b7a81eea0205bece68cd05"></a><a id="t.0"></a><table class="c41"><tr class="c5"><td class="c30" colspan="1" rowspan="1"><p class="c17"><span class="c0">Type of network</span></p></td><td class="c3" colspan="1" rowspan="1"><p class="c17"><span class="c0"># of network layers</span></p></td><td class="c18" colspan="1" rowspan="1"><p class="c17"><span class="c0"># of weights</span></p></td><td class="c44" colspan="1" rowspan="1"><p class="c17"><span class="c0">% of deployed</span></p></td></tr><tr class="c5"><td class="c30" colspan="1" rowspan="1"><p class="c17"><span class="c0">MLP0</span></p></td><td class="c3" colspan="1" rowspan="1"><p class="c17"><span class="c0">5</span></p></td><td class="c18" colspan="1" rowspan="1"><p class="c17"><span class="c0">20M</span></p></td><td class="c44" colspan="1" rowspan="1"><p class="c17"><span class="c0">61%</span></p></td></tr><tr class="c5"><td class="c30" colspan="1" rowspan="1"><p class="c17"><span class="c0">MLP1</span></p></td><td class="c3" colspan="1" 
rowspan="1"><p class="c17"><span class="c0">4</span></p></td><td class="c18" colspan="1" rowspan="1"><p class="c17"><span class="c0">5M</span></p></td><td class="c44" colspan="1" rowspan="1"><p class="c6"><span class="c0"></span></p></td></tr><tr class="c5"><td class="c30" colspan="1" rowspan="1"><p class="c17"><span class="c0">LSTM0</span></p></td><td class="c3" colspan="1" rowspan="1"><p class="c17"><span class="c0">58</span></p></td><td class="c18" colspan="1" rowspan="1"><p class="c17"><span class="c0">52M</span></p></td><td class="c44" colspan="1" rowspan="1"><p class="c17"><span class="c0">29%</span></p></td></tr><tr class="c5"><td class="c30" colspan="1" rowspan="1"><p class="c17"><span class="c0">LSTM1</span></p></td><td class="c3" colspan="1" rowspan="1"><p class="c17"><span class="c0">56</span></p></td><td class="c18" colspan="1" rowspan="1"><p class="c17"><span class="c0">34M</span></p></td><td class="c44" colspan="1" rowspan="1"><p class="c6"><span class="c0"></span></p></td></tr><tr class="c5"><td class="c30" colspan="1" rowspan="1"><p class="c17"><span class="c0">CNN0</span></p></td><td class="c3" colspan="1" rowspan="1"><p class="c17"><span class="c0">16</span></p></td><td class="c18" colspan="1" rowspan="1"><p class="c17"><span class="c0">8M</span></p></td><td class="c44" colspan="1" rowspan="1"><p class="c17"><span class="c0">5%</span></p></td></tr><tr class="c5"><td class="c30" colspan="1" rowspan="1"><p class="c17"><span class="c0">CNN1</span></p></td><td class="c3" colspan="1" rowspan="1"><p class="c17"><span class="c0">89</span></p></td><td class="c18" colspan="1" rowspan="1"><p class="c17"><span class="c0">100M</span></p></td><td class="c44" colspan="1" rowspan="1"><p class="c6"><span class="c0"></span></p></td></tr><tr class="c5"><td class="c30" colspan="1" rowspan="1"><p class="c6"><span class="c0"></span></p><p class="c6"><span class="c0"></span></p></td><td class="c3" colspan="1" rowspan="1"><p class="c6"><span class="c0"></span></p></td><td class="c18" colspan="1" rowspan="1"><p class="c6"><span class="c0"></span></p></td><td class="c44" colspan="1" rowspan="1"><p class="c6"><span class="c0"></span></p></td></tr></table><p class="c34 c10"><span class="c0">As you can see in the table, the number of weights in each neural network varies from 5 million to 100 million. Every single prediction requires many steps of multiplying processed input data by a weight matrix and applying an activation function.</span></p><p class="c34 c10"><span class="c0">In total, this is a massive amount of computation. As a first optimization, rather than executing all of these mathematical operations with ordinary 32-bit or 16-bit floating point operations on CPUs or GPUs, we apply a technique called quantization that allows us to work with integer operations instead. This enables us to reduce the total amount of memory and computing resources required to make useful predictions with our neural network models.</span></p><h1 class="c25 c10" id="h.27aa6qxv0z9s"><span class="c40 c15">PARALLEL PROCESSING ON THE MATRIX MULTIPLIER UNIT</span></h1><p class="c1"><span>Typical RISC processors provide instructions for simple calculations, such as multiplying or adding numbers. These are so-called </span><span class="c15">scalar processors</span><span class="c0">, as they process a single operation (= scalar operation) with each instruction.</span></p><p class="c34 c10"><span>Even though CPUs run at clock speeds in the gigahertz range, it can still take a long time to execute large matrix operations via a sequence of scalar operations. One effective and well-known way to improve the performance of such large matrix operations is through </span><span class="c15">vector processing</span><span>, where the same operation is performed concurrently across numerous data elements at the same time. CPUs incorporate instruction set extensions such as</span><span><a class="c9" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Streaming_SIMD_Extensions&amp;sa=D&amp;source=editors&amp;ust=1654780423475306&amp;usg=AOvVaw2WPXEpUmgQscRQaJVk8SQE">&nbsp;</a></span><span class="c33"><a class="c9" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Streaming_SIMD_Extensions&amp;sa=D&amp;source=editors&amp;ust=1654780423475574&amp;usg=AOvVaw2ArB2ULiSb8Tzw--Afc4H4">SSE</a></span><span>&nbsp;and</span><span><a class="c9" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Advanced_Vector_Extensions&amp;sa=D&amp;source=editors&amp;ust=1654780423475744&amp;usg=AOvVaw2RjEFjcS7yl0UMtW3oVHKT">&nbsp;</a></span><span class="c33"><a class="c9" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Advanced_Vector_Extensions&amp;sa=D&amp;source=editors&amp;ust=1654780423475906&amp;usg=AOvVaw2gcepBVGGedYyonLz7qkIs">AVX</a></span><span class="c0">&nbsp;that express such vector operations. The streaming multiprocessors (SMs) of GPUs are effectively vector processors, with many such SMs on a single GPU die. Machines with vector processing support can process hundreds to thousands of operations in a single clock cycle.</span></p><p class="c34 c10"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 194.50px;"><img alt="" src="/images/image17.png" style="width: 624.00px; height: 194.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c10 c19"><span class="c29">Fig6. Scalar, vector and matrix</span></p><p class="c34 c10"><span>In the case of the TPU, Google designed its MXU as a </span><span class="c15">matrix processor</span><span>&nbsp;that processes </span><span class="c15">hundreds of thousands of operations (= matrix operation)</span><span class="c0">&nbsp;in a single clock cycle. Think of it like printing documents one character at a time, one line at a time and a page at a time.</span></p><p class="c1 c28"><span class="c40 c15"></span></p><p class="c1"><span class="c40 c15">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; The heart of the TPU: A systolic array</span></p><p class="c1 c28"><span class="c40 c15"></span></p><p class="c1"><span>To implement such a large-scale matrix processor, the MXU features a drastically different architecture than typical CPUs and GPUs, called a</span><span><a class="c9" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Systolic_array&amp;sa=D&amp;source=editors&amp;ust=1654780423476549&amp;usg=AOvVaw2yvXPsWs1HzxtOODFtl4jt">&nbsp;</a></span><span class="c33"><a class="c9" href="https://www.google.com/url?q=https://en.wikipedia.org/wiki/Systolic_array&amp;sa=D&amp;source=editors&amp;ust=1654780423476695&amp;usg=AOvVaw3vcerkDQB1g9h_-nsxjQpU">systolic array</a></span><span>. CPUs are designed to run almost any calculation; they&#39;re general-purpose computers. To implement this generality, CPUs store values in registers, and a program tells the Arithmetic Logic Units (ALUs) which registers to read, the operation to perform (such as an addition, multiplication or logical AND) and the register into which to put the result. A program consists of a sequence of these read/operate/write operations. All of these features that support generality (registers, ALUs and programmed control) have costs in terms of power and chip area</span><span class="c40 c15">.</span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 500.00px; height: 412.00px;"><img alt="" src="/images/image11.png" style="width: 500.00px; height: 412.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1"><span class="c29">Fig 7. CPUs and GPUs often spend energy to access multiple registers per operation.</span></p><p class="c17 c10 c37"><span class="c29">&nbsp;A systolic array chains multiple ALUs together, reusing the result of reading a single register.</span></p><p class="c1 c28"><span class="c0"></span></p><p class="c17 c10"><span class="c0">For an MXU, however, matrix multiplication reuses both inputs many times as part of producing the output. We can read each input value once, but use it for many different operations without storing it back to a register. Wires only connect spatially adjacent ALUs, which makes them short and energy-efficient. The ALUs perform only multiplications and additions in fixed patterns, which simplifies their design.</span></p><p class="c17 c10"><span class="c0">The design is called systolic because the data flows through the chip in waves, reminiscent of the way that the heart pumps blood. The particular kind of systolic array in the MXU is optimized for power and area efficiency in performing matrix multiplications, and is not well suited for general-purpose computation. It makes an engineering tradeoff: limiting registers, control and operational flexibility in exchange for efficiency and much higher operation density.</span></p><p class="c6 c10"><span class="c0"></span></p><p class="c17 c10"><span>The TPU Matrix Multiplication Unit has a systolic array mechanism that contains 256 &times; 256 = total 65,536 ALUs. That means a TPU can process 65,536 multiply-and-adds for 8-bit integers every cycle. Because a TPU runs at 700MHz, a TPU can compute 65,536 &times; 700,000,000 = 46 &times; 10</span><span class="c26">12</span><span>&nbsp;multiply-and-add operations or 92 Teraops per second (92 &times; 10</span><span class="c26">12</span><span class="c0">) in the matrix unit.</span></p><p class="c6 c10"><span class="c0"></span></p><a id="t.
82ddf4d1e498a5c7a58e73ae6d350db97665fb6f"></a><a id="t.1"></a><table class="c41"><tr class="c5"><td class="c46" colspan="1" rowspan="1"><p class="c6"><span class="c0"></span></p></td><td class="c23" colspan="1" rowspan="1"><p class="c17"><span class="c0">Operations per cycle</span></p></td></tr><tr class="c5"><td class="c46" colspan="1" rowspan="1"><p class="c17"><span class="c0">CPU</span></p></td><td class="c23" colspan="1" rowspan="1"><p class="c17"><span class="c0">a few</span></p></td></tr><tr class="c5"><td class="c46" colspan="1" rowspan="1"><p class="c17"><span class="c0">CPU (vector extension)</span></p></td><td class="c23" colspan="1" rowspan="1"><p class="c17"><span class="c0">tens</span></p></td></tr><tr class="c5"><td class="c46" colspan="1" rowspan="1"><p class="c17"><span class="c0">GPU</span></p></td><td class="c23" colspan="1" rowspan="1"><p class="c17"><span class="c0">tens of thousands</span></p></td></tr><tr class="c5"><td class="c46" colspan="1" rowspan="1"><p class="c17"><span class="c0">TPU</span></p></td><td class="c23" colspan="1" rowspan="1"><p class="c17"><span class="c0">hundreds of thousands, up to 128K</span></p></td></tr></table><p class="c34 c10 c28"><span class="c0"></span></p><p class="c34 c10"><span class="c0">In comparison, a typical RISC CPU without vector extensions can only execute just one or two arithmetic operations per instruction, and GPUs can execute thousands of operations per instruction. With the TPU, a single cycle of a MatrixMultiply instruction can invoke hundreds of thousands of operations.</span></p><p class="c34 c10"><span>During the execution of this massive matrix multiply, all intermediate results are passed directly between 64K ALUs without any memory access, significantly reducing power consumption and increasing throughput. As a result, the CISC-based matrix processor design delivers an outstanding performance-per-watt ratio: TPU provides a </span><span class="c15">83X</span><span>&nbsp;better ratio compared with contemporary CPUs and a </span><span class="c15">29X</span><span class="c0">&nbsp;better ratio than contemporary GPUs.</span></p><p class="c10 c34"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 624.00px; height: 172.50px;"><img alt="" src="/images/image14.png" style="width: 624.00px; height: 172.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c34 c10"><span class="c29">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Fig 8. Performance / watt, relative to contemporary CPUs and GPUs (in log scale)(Incremental, weighted mean)</span></p><h1 class="c25 c10" id="h.7jov1hjn7dqy"><span class="c40 c15">INSTRUCTION SET</span></h1><p class="c1"><span class="c21">A computer-implemented method that includes receiving, by a processing unit, an instruction that specifies data values for performing a tensor computation . In response to receiving the instruction, themethodmay include, performing, by the processing unit, the tensor computation by executing a loop nest comprising a plurality of loops, wherein a structure of the loop nest is defined based on one or more of the data values of the instruction. The tensor computation can be at least a portion of a computation of a neural network layer. The data values specified by the instruction may comprise a value that specifies a type of the neural network layer, and the structure of the loop nest can be defined at least in part by the type of the neural network layer.</span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 474.00px; height: 549.50px;"><img alt="" src="/images/image8.png" style="width: 474.00px; height: 549.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1 c28"><span class="c21"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 530.00px; height: 426.50px;"><img alt="" src="/images/image3.png" style="width: 530.00px; height: 426.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c1 c28"><span class="c21"></span></p><p class="c1"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 567.00px; height: 395.50px;"><img alt="" src="/images/image6.png" style="width: 567.00px; height: 395.50px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h1 class="c25 c10" id="h.pmuungyypamk"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 619.00px; height: 512.00px;"><img alt="" src="/images/image15.png" style="width: 619.00px; height: 512.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></h1><h1 class="c25 c10 c55" id="h.tv410dtpdrzx"><span class="c0">FLOW DIAGRAM</span></h1><p class="c10 c24"><span class="c0"></span></p><p class="c1"><span class="c32">A hardware accelerated having an efficient instruction set is </span><span class="c32">disclosed . An apparatus may comprise logic configured to access a first and a second machine instruction . The second machine instruction may be missing a tensor operand needed to execute the second machine instruction. The logic may be further configured to execute the first machine instruction, resulting in a tensor. The logic may be further configured to execute the second machine instruction using the resultant tensor as the missing tensor operand.</span></p><p class="c49 c10"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 567.50px; height: 826.00px;"><img alt="" src="/images/image1.png" style="width: 567.50px; height: 826.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c10 c49"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 569.50px; height: 514.24px;"><img alt="" src="/images/image2.png" style="width: 569.50px; height: 514.24px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><h1 class="c25 c10" id="h.utm1nltfrax0"><span class="c0">CONCLUSION</span></h1><p class="c1"><span class="c32">The TPU succeeded because of the large&mdash;but not too large&mdash;matrix multiply unit; the substantial software-controlled on-chip memory; the ability to run whole inference models to reduce dependence on host CPU; a single-threaded, deterministic execution model that proved to be a good match to 99th-percentile response time limits; enough flexibility to match the NNs of 2017 as well as of 2013; the omission of general-purpose features that enabled a small and low power die despite the larger data path and memory; the use of 8-bit integers by the quantized applications; and that applications were written using TensorFlow, which made it easy to port them to the TPU at high-performance rather than them having to be rewritten to run well on the very different TPU hardware. Order-of-magnitude differences between commercial products are rare in computer architecture, which may lead to the TPU becoming an archetype for domain-specific architectures. We expect that many will build successors that will raise the bar even higher</span></p><p class="c1 c28"><span class="c0"></span></p><h1 class="c1 c28"><span class="c0"></span></p><span class="c0">Works Cited</span></h1><p class="c17 c16 c10"><span class="c36">Norman P. Jouppi, Cliff Young, Nishant Patil</span><span class="c48 c54">, </span><span class="c8">&nbsp;</span><span class="c36">In-Datacenter Performance Analysis of a Tensor Processing Unit</span><span class="c7">, 2017.</span></p><p class="c6 c16 c10"><span class="c7"></span></p><p class="c17 c16 c10"><span class="c33 c48"><a class="c9" href="https://www.google.com/url?q=https://patentimages.storage.googleapis.com/78/54/ba/c1e8901029486c/US20180341484A1.pdf&amp;sa=D&amp;source=editors&amp;ust=1654780423482399&amp;usg=AOvVaw1dlOdkjc8sIO2i3GRmJDFI">https://patentimages.storage.googleapis.com/78/54/ba/c1e8901029486c/US20180341484A1.pdf</a></span></p><p class="c6 c16 c10"><span class="c7"></span></p><p class="c17 c16 c10"><span class="c33 c48"><a class="c9" href="https://www.google.com/url?q=https://patentimages.storage.googleapis.com/11/74/c4/8390c55ced8356/US9836691.pdf&amp;sa=D&amp;source=editors&amp;ust=1654780423482870&amp;usg=AOvVaw2ueYXWbnid87uxKr8x4Eyw">https://patentimages.storage.googleapis.com/11/74/c4/8390c55ced8356/US9836691.pdf</a></span></
p><p class="c6 c16 c10"><span class="c7"></span></p><p class="c6 c10"><span class="c7"></span></p><p class="c17 c16 c10"><span class="c4">Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G.S., Davis, A., Dean, J., Devin, M. and Ghemawat, S., 2016. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv:1603.04467</span></p><p class="c17 c16 c10"><span class="c48">&nbsp;</span><span class="c0">&nbsp;</span></p><p class="c17 c16 c10"><span class="c36">Jouppi, N. May 18, 2016. Google supercharges machine learning tasks with TPU custom chip. </span><span class="c14"><a class="c9" href="https://www.google.com/url?q=https://cloudplatform.googleblog.com&amp;sa=D&amp;source=editors&amp;ust=1654780423483442&amp;usg=AOvVaw2rXHYsTvEsGBlvzoXSh1pH">https://cloudplatform.googleblog.com</a></span></p><p class="c6 c16 c10"><span class="c22"></span></p><p class="c17 c16 c10"><span class="c36">Hennessy, J.L. and Patterson, D.A., 2018. </span><span class="c36 c39">Computer architecture: a quantitative approach</span><span class="c36">, 6th edition, Elsevier</span></p><p class="c6 c16 c10"><span class="c20"></span></p><p class="c17 c16 c10"><span class="c36">Shiva ram, </span><span class="c14"><a class="c9" href="https://www.google.com/url?q=https://pages.cs.wisc.edu/~shivaram/cs744-fa21-slides/cs744-tpu-notes.pdf&amp;sa=D&amp;source=editors&amp;ust=1654780423484187&amp;usg=AOvVaw2HC9ZEAQRZamOc8aXhm69E">https://pages.cs.wisc.edu/~shivaram/cs744-fa21-slides/cs744-tpu-notes.pdf</a></span><span class="c22">.</span></p><p class="c6 c16 c10"><span class="c22"></span></p><p class="c17 c16 c10"><span class="c14"><a class="c9" href="https://www.google.com/url?q=https://www.kaggle.com/docs/tpu&amp;sa=D&amp;source=editors&amp;ust=1654780423484670&amp;usg=AOvVaw3jhpgY80NERocNKT-V-B0q">https://www.kaggle.com/docs/tpu</a></span></p><p class="c6 c16 c10"><span class="c22"></span></p><p class="c17 c16 c10"><span class="c14"><a class="c9" href="https://www.google.com/url?q=https://cloud.google.com/tpu/docs/system-architecture-tpu-vm&amp;sa=D&amp;source=editors&amp;ust=1654780423485090&amp;usg=AOvVaw1EEMzl4Krlax1DeyquIapq">https://cloud.google.com/tpu/docs/system-architecture-tpu-vm</a></span></p><p class="c6 c16 c10"><span class="c22"></span></p><p class="c17 c16 c10"><span class="c14"><a class="c9" href="https://www.google.com/url?q=https://www.tomshardware.com/news/google-tensor-processing-unit-machine-learning,31834.html&amp;sa=D&amp;source=editors&amp;ust=1654780423485527&amp;usg=AOvVaw0GMH5AkLqc6lSVqzg_ZK4V">https://www.tomshardware.com/news/google-tensor-processing-unit-machine-learning,31834.html</a></span></p><p class="c6 c10 c16"><span class="c22"></span></p><p class="c6 c16 c10"><span class="c22"></span></p><p class="c6 c16 c10"><span class="c22"></span></p><p class="c6 c16 c10"><span class="c0"></span></p>

# PDF_Version --> [TPU](/TPUs-2.pdf)

